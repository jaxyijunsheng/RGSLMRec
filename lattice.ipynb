{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b21fa-fc4f-4626-957c-313551fed33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    class Args:\n",
    "        p=0\n",
    "        data_path = './'\n",
    "        seed = 123\n",
    "        dataset = 'beauty'\n",
    "        verbose = 5\n",
    "        epoch = 2000\n",
    "        batch_size = 1024\n",
    "        regs = '[1e-5,1e-5,1e-2]'\n",
    "        lr = 0.0005\n",
    "        model_name = 'lattice'\n",
    "        embed_size = 64\n",
    "        feat_embed_dim = 64\n",
    "        weight_size = '[64,64]'\n",
    "        core = 5\n",
    "        topk = 10\n",
    "        lambda_coeff = 0.9\n",
    "        cf_model = 'lightgcn'\n",
    "        n_layers = 1\n",
    "        mess_dropout = '[0.1, 0.1]'\n",
    "        early_stopping_patience = 7\n",
    "        gpu_id = 0\n",
    "        Ks = '[10, 20, 50]'\n",
    "        test_flag = 'part'\n",
    "        shuffle='text'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79832fce-f477-487b-b9c3-ad0731ba2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if all_pos_num == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sum(r) / all_pos_num\n",
    "\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def mrr_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    #print(r)\n",
    "    if np.sum(r) > 0:\n",
    "        #print(1/(np.where(r==1.0)[0]+1).astype(float)[0])\n",
    "        return 1/(np.where(r==1.0)[0]+1).astype(float)[0]\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf065193-5b51-4201-9341-c04afd556689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "import json\n",
    "#from utility.parser import parse_args\n",
    "args = parse_args()\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path + '/%d-core' % args.core\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/%d-core/train.json' % (args.core)\n",
    "        val_file = path + '/%d-core/val.json' % (args.core)\n",
    "        test_file = path + '/%d-core/test.json'  % (args.core)\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        train = json.load(open(train_file))\n",
    "        test = json.load(open(test_file))\n",
    "        val = json.load(open(val_file))\n",
    "        for uid, items in train.items():\n",
    "            if len(items) == 0:\n",
    "                continue\n",
    "            uid = int(uid)\n",
    "            self.exist_users.append(uid)\n",
    "            self.n_items = max(self.n_items, max(items))\n",
    "            self.n_users = max(self.n_users, uid)\n",
    "            self.n_train += len(items)\n",
    "\n",
    "        for uid, items in test.items():\n",
    "            uid = int(uid)\n",
    "            try:\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "                self.n_users = max(self.n_users, uid)\n",
    "                self.n_test += len(items)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for uid, items in val.items():\n",
    "            uid = int(uid)\n",
    "            try:\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "                self.n_users = max(self.n_users, uid)\n",
    "                self.n_val += len(items)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set, self.val_set = {}, {}, {}\n",
    "        for uid, train_items in train.items():\n",
    "            if len(train_items) == 0:\n",
    "                continue\n",
    "            uid = int(uid)\n",
    "            for idx, i in enumerate(train_items):\n",
    "                self.R[uid, i] = 1.\n",
    "\n",
    "            self.train_items[uid] = train_items\n",
    "\n",
    "        self.my_test_set=[]\n",
    "        for uid, test_items in test.items():\n",
    "            uid = int(uid)\n",
    "            if len(test_items) == 0:\n",
    "                continue\n",
    "            for i in test_items:\n",
    "                self.my_test_set.append([uid,i])\n",
    "            try:\n",
    "                self.test_set[uid] = test_items\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for uid, val_items in val.items():\n",
    "            uid = int(uid)\n",
    "            if len(val_items) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                self.val_set[uid] = val_items\n",
    "            except:\n",
    "                continue            \n",
    "\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def get_D_inv(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            return d_mat_inv\n",
    "\n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "\n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "            return temp\n",
    "\n",
    "        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "        # users = self.exist_users[:]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "            # neg_items += sample_neg_items_for_u(u, 3)\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4ccc8-9f4c-4647-9931-744be7acc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utility.metrics as metrics\n",
    "#from utility.parser import parse_args\n",
    "#from utility.load_data import Data\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "cores = multiprocessing.cpu_count() // 5\n",
    "\n",
    "args = parse_args()\n",
    "Ks = eval(args.Ks)\n",
    "\n",
    "data_generator = Data(path=args.data_path + args.dataset, batch_size=args.batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = metrics.auc(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio, mrr = [], [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "        mrr.append(mrr_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc, 'mrr': np.array(mrr)}\n",
    "\n",
    "\n",
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    is_val = x[-1]\n",
    "    rating = x[0]\n",
    "    #uid\n",
    "    u = x[1]\n",
    "    #user u's items in the training set\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    #user u's items in the test set\n",
    "    if is_val:\n",
    "        user_pos_test = data_generator.val_set[u]\n",
    "    else:\n",
    "        user_pos_test = data_generator.test_set[u]\n",
    "\n",
    "    all_items = set(range(ITEM_NUM))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    if args.test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "\n",
    "def test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val, drop_flag=False, batch_test_flag=False):\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)),'mrr': np.zeros(len(Ks)), 'auc': 0.}\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "        user_batch = test_users[start: end]\n",
    "        if batch_test_flag:\n",
    "            n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
    "            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "            i_count = 0\n",
    "            for i_batch_id in range(n_item_batchs):\n",
    "                i_start = i_batch_id * i_batch_size\n",
    "                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "                item_batch = range(i_start, i_end)\n",
    "                u_g_embeddings = ua_embeddings[user_batch]\n",
    "                i_g_embeddings = ia_embeddings[item_batch]\n",
    "                i_rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
    "\n",
    "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "                i_count += i_rate_batch.shape[1]\n",
    "\n",
    "            assert i_count == ITEM_NUM\n",
    "\n",
    "        else:\n",
    "            item_batch = range(ITEM_NUM)\n",
    "            u_g_embeddings = ua_embeddings[user_batch]\n",
    "            #print(max(item_batch))\n",
    "            i_g_embeddings = ia_embeddings[item_batch]\n",
    "            rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
    "\n",
    "        rate_batch = rate_batch.detach().cpu().numpy()\n",
    "        user_batch_rating_uid = zip(rate_batch, user_batch, [is_val] * len(user_batch))\n",
    "\n",
    "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision'] / n_test_users\n",
    "            result['recall'] += re['recall'] / n_test_users\n",
    "            result['ndcg'] += re['ndcg'] / n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "            result['auc'] += re['auc'] / n_test_users\n",
    "            result['mrr'] += re['mrr'] / n_test_users\n",
    "\n",
    "    assert count == n_test_users\n",
    "    pool.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53258d-b2dc-46f3-844a-0e2cf50cb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.sparse as sparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from utility.parser import parse_args\n",
    "args = parse_args()\n",
    "\n",
    "def build_knn_neighbourhood(adj, topk):\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "    return weighted_adjacency_matrix\n",
    "def compute_normalized_laplacian(adj):\n",
    "    rowsum = torch.sum(adj, -1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "    L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "    return L_norm\n",
    "def build_sim(context):\n",
    "    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n",
    "    sim = torch.mm(context_norm, context_norm.transpose(1, 0))\n",
    "    return sim\n",
    "\n",
    "class LATTICE(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weight_size = weight_size\n",
    "        self.n_ui_layers = len(self.weight_size)\n",
    "        self.weight_size = [self.embedding_dim] + self.weight_size\n",
    "        self.user_embedding = nn.Embedding(n_users, self.embedding_dim)\n",
    "        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n",
    "\n",
    "        if args.cf_model == 'ngcf':\n",
    "            self.GC_Linear_list = nn.ModuleList()\n",
    "            self.Bi_Linear_list = nn.ModuleList()\n",
    "            self.dropout_list = nn.ModuleList()\n",
    "            for i in range(self.n_ui_layers):\n",
    "                self.GC_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
    "                self.Bi_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
    "                self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
    "\n",
    "\n",
    "        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False)\n",
    "        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False)\n",
    "            \n",
    "    \n",
    "        #if os.path.exists('./%s/%s-core/image_adj_%d.pt'%(args.dataset, args.core, args.topk)):\n",
    "            #image_adj = torch.load('./%s/%s-core/image_adj_%d.pt'%(args.dataset, args.core, args.topk))\n",
    "        #else:\n",
    "        image_adj = build_sim(self.image_embedding.weight.detach())\n",
    "        image_adj = build_knn_neighbourhood(image_adj, topk=args.topk)\n",
    "        image_adj = compute_normalized_laplacian(image_adj)\n",
    "        #torch.save(image_adj, './%s/%s-core/image_adj_%d.pt'%(args.dataset, args.core, args.topk))\n",
    "\n",
    "        #if os.path.exists('./%s/%s-core/text_adj_%d.pt'%(args.dataset, args.core, args.topk)):\n",
    "        #    text_adj = torch.load('./%s/%s-core/text_adj_%d.pt'%(args.dataset, args.core, args.topk))        \n",
    "        #else:\n",
    "        text_adj = build_sim(self.text_embedding.weight.detach())\n",
    "        text_adj = build_knn_neighbourhood(text_adj, topk=args.topk)\n",
    "        text_adj = compute_normalized_laplacian(text_adj)\n",
    "        #torch.save(text_adj, './%s/%s-core/text_adj_%d.pt'%(args.dataset, args.core, args.topk))\n",
    "\n",
    "        self.text_original_adj = text_adj.cuda()\n",
    "        self.image_original_adj = image_adj.cuda()\n",
    "        \n",
    "        self.image_trs = nn.Linear(image_feats.shape[1], args.feat_embed_dim)\n",
    "        self.text_trs = nn.Linear(text_feats.shape[1], args.feat_embed_dim)\n",
    "\n",
    "\n",
    "        self.modal_weight = nn.Parameter(torch.Tensor([0.5, 0.5]))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, adj, build_item_graph=False):\n",
    "        image_feats = self.image_trs(self.image_embedding.weight)\n",
    "        text_feats = self.text_trs(self.text_embedding.weight)\n",
    "        if build_item_graph:\n",
    "            weight = self.softmax(self.modal_weight)\n",
    "            self.image_adj = build_sim(image_feats)\n",
    "            self.image_adj = build_knn_neighbourhood(self.image_adj, topk=args.topk)\n",
    "\n",
    "            self.text_adj = build_sim(text_feats)\n",
    "            self.text_adj = build_knn_neighbourhood(self.text_adj, topk=args.topk)   \n",
    "\n",
    "            \n",
    "            learned_adj = weight[0] * self.image_adj + weight[1] * self.text_adj\n",
    "            learned_adj = compute_normalized_laplacian(learned_adj)\n",
    "            original_adj = weight[0] * self.image_original_adj + weight[1] * self.text_original_adj\n",
    "            self.item_adj = (1 - args.lambda_coeff) * learned_adj + args.lambda_coeff * original_adj\n",
    "        else:\n",
    "            self.item_adj = self.item_adj.detach()\n",
    "\n",
    "        h = self.item_id_embedding.weight\n",
    "        for i in range(args.n_layers):\n",
    "            h = torch.mm(self.item_adj, h)\n",
    "\n",
    "        if args.cf_model == 'ngcf':\n",
    "            ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n",
    "            all_embeddings = [ego_embeddings]\n",
    "            for i in range(self.n_ui_layers):\n",
    "                side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n",
    "                sum_embeddings = F.leaky_relu(self.GC_Linear_list[i](side_embeddings))\n",
    "                bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "                bi_embeddings = F.leaky_relu(self.Bi_Linear_list[i](bi_embeddings))\n",
    "                ego_embeddings = sum_embeddings + bi_embeddings\n",
    "                ego_embeddings = self.dropout_list[i](ego_embeddings)\n",
    "\n",
    "                norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "                all_embeddings += [norm_embeddings]\n",
    "\n",
    "            all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)            \n",
    "            u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n",
    "            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n",
    "            return u_g_embeddings, i_g_embeddings\n",
    "        elif args.cf_model == 'lightgcn':\n",
    "            ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n",
    "            all_embeddings = [ego_embeddings]\n",
    "            for i in range(self.n_ui_layers):\n",
    "                side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n",
    "                ego_embeddings = side_embeddings\n",
    "                all_embeddings += [ego_embeddings]\n",
    "            all_embeddings = torch.stack(all_embeddings, dim=1)\n",
    "            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)\n",
    "            u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n",
    "            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n",
    "            return u_g_embeddings, i_g_embeddings\n",
    "        elif args.cf_model == 'mf':\n",
    "                return self.user_embedding.weight, self.item_id_embedding.weight + F.normalize(h, p=2, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683a438-f619-4919-93df-25e6106b3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.sparse as sparse\n",
    "\n",
    "#from utility.parser import parse_args\n",
    "#from Models import LATTICE\n",
    "#from utility.batch_test import *\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, data_config):\n",
    "        # argument settings\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "\n",
    "        self.model_name = args.model_name\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.lr = args.lr\n",
    "        self.emb_dim = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.weight_size = eval(args.weight_size)\n",
    "        self.n_layers = len(self.weight_size)\n",
    "        self.regs = eval(args.regs)\n",
    "        self.decay = self.regs[0]\n",
    "\n",
    "        self.norm_adj = data_config['norm_adj']\n",
    "        self.norm_adj = self.sparse_mx_to_torch_sparse_tensor(self.norm_adj).float().cuda()\n",
    "        '''\n",
    "        if args.p==0:\n",
    "            image_feats = np.load('./{}/image_feat.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/text_feat.npy'.format(args.dataset))\n",
    "            print('load ./{}/image_feat.npy'.format(args.dataset))\n",
    "        else:\n",
    "            image_feats = np.load('./{}/shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "        '''\n",
    "        if args.shuffle=='all':\n",
    "            image_feats = np.load('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='text':\n",
    "            image_feats = np.load('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='image':\n",
    "            image_feats = np.load('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        else:\n",
    "            image_feats = np.load('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        \n",
    "        self.model = LATTICE(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, image_feats, text_feats)\n",
    "        self.model = self.model.cuda()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.lr_scheduler = self.set_lr_scheduler()\n",
    "\n",
    "    def set_lr_scheduler(self):\n",
    "        fac = lambda epoch: 0.96 ** (epoch / 50)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)\n",
    "        return scheduler\n",
    "\n",
    "    def test(self, users_to_test, is_val):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            ua_embeddings, ia_embeddings = self.model(self.norm_adj, build_item_graph=True)\n",
    "        #print(ia_embeddings.shape)\n",
    "        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n",
    "        #result = my_test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n",
    "        return result\n",
    "\n",
    "    def train(self):\n",
    "        training_time_list = []\n",
    "        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger,mrr_loger = [], [], [], [], [], []\n",
    "        stopping_step = 0\n",
    "        should_stop = False\n",
    "        cur_best_pre_0 = 0.\n",
    "\n",
    "        n_batch = data_generator.n_train // args.batch_size + 1\n",
    "        best_recall = 0\n",
    "        for epoch in (range(args.epoch)):\n",
    "            t1 = time()\n",
    "            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
    "            n_batch = data_generator.n_train // args.batch_size + 1\n",
    "            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.\n",
    "            sample_time = 0.\n",
    "            build_item_graph = True\n",
    "            for idx in (range(n_batch)):\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                sample_t1 = time()\n",
    "                users, pos_items, neg_items = data_generator.sample()\n",
    "                sample_time += time() - sample_t1                                                 \n",
    "                ua_embeddings, ia_embeddings = self.model(self.norm_adj, build_item_graph=build_item_graph)\n",
    "                build_item_graph = False\n",
    "                u_g_embeddings = ua_embeddings[users]\n",
    "                pos_i_g_embeddings = ia_embeddings[pos_items]\n",
    "                neg_i_g_embeddings = ia_embeddings[neg_items]\n",
    "\n",
    "\n",
    "                batch_mf_loss, batch_emb_loss, batch_reg_loss = self.bpr_loss(u_g_embeddings, pos_i_g_embeddings,\n",
    "                                                                              neg_i_g_embeddings)\n",
    "\n",
    "                batch_loss = batch_mf_loss + batch_emb_loss + batch_reg_loss\n",
    "\n",
    "                batch_loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                loss += float(batch_loss)\n",
    "                mf_loss += float(batch_mf_loss)\n",
    "                emb_loss += float(batch_emb_loss)\n",
    "                reg_loss += float(batch_reg_loss)\n",
    "\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "            del ua_embeddings, ia_embeddings, u_g_embeddings, neg_i_g_embeddings, pos_i_g_embeddings\n",
    "\n",
    "            if math.isnan(loss) == True:\n",
    "                print('ERROR: loss is nan.')\n",
    "                sys.exit()\n",
    "\n",
    "            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (\n",
    "                epoch, time() - t1, loss, mf_loss, emb_loss)\n",
    "            training_time_list.append(time() - t1)\n",
    "            print(perf_str)\n",
    "\n",
    "            if epoch % args.verbose != 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            t2 = time()\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            users_to_val = list(data_generator.val_set.keys())\n",
    "            ret = self.test(users_to_val, is_val=True)\n",
    "            training_time_list.append(t2 - t1)\n",
    "\n",
    "            t3 = time()\n",
    "\n",
    "            loss_loger.append(loss)\n",
    "            rec_loger.append(ret['recall'])\n",
    "            pre_loger.append(ret['precision'])\n",
    "            ndcg_loger.append(ret['ndcg'])\n",
    "            hit_loger.append(ret['hit_ratio'])\n",
    "            mrr_loger.append(ret['mrr'])\n",
    "            if args.verbose > 0:\n",
    "                perf_str = 'Epoch %d [%.1fs + %.1fs]:  val==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
    "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], mrr=[%.5f, %.5f]' % \\\n",
    "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0],\n",
    "                            ret['recall'][1],\n",
    "                            ret['precision'][0], ret['precision'][1], ret['hit_ratio'][0], ret['hit_ratio'][1],\n",
    "                            ret['ndcg'][0], ret['ndcg'][1],ret['mrr'][0], ret['mrr'][1])\n",
    "                print(perf_str)\n",
    "\n",
    "            if ret['recall'][1] > best_recall:\n",
    "                best_recall = ret['recall'][1]\n",
    "                test_ret = self.test(users_to_test, is_val=False)\n",
    "                perf_str = 'Epoch %d [%.1fs + %.1fs]: test==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
    "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], mrr=[%.5f, %.5f]' % \\\n",
    "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, test_ret['recall'][0],\n",
    "                            test_ret['recall'][1],\n",
    "                            test_ret['precision'][0], test_ret['precision'][1], test_ret['hit_ratio'][0], test_ret['hit_ratio'][1],\n",
    "                            test_ret['ndcg'][0], test_ret['ndcg'][1],test_ret['mrr'][0], test_ret['mrr'][1])\n",
    "                print(perf_str)                \n",
    "                stopping_step = 0\n",
    "            elif stopping_step < args.early_stopping_patience:\n",
    "                stopping_step += 1\n",
    "                print('#####Early stopping steps: %d #####' % stopping_step)\n",
    "            else:\n",
    "                print('#####Early stop! #####')\n",
    "                break\n",
    "\n",
    "        print(test_ret)\n",
    "        print(args.dataset,args.shuffle,args.p,test_ret, file=open('test_lattice_ret.txt','a'))\n",
    "        \n",
    "    def bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n",
    "\n",
    "        regularizer = 1./2*(users**2).sum() + 1./2*(pos_items**2).sum() + 1./2*(neg_items**2).sum()\n",
    "        regularizer = regularizer / self.batch_size\n",
    "\n",
    "        maxi = F.logsigmoid(pos_scores - neg_scores)\n",
    "        mf_loss = -torch.mean(maxi)\n",
    "\n",
    "        emb_loss = self.decay * regularizer\n",
    "        reg_loss = 0.0\n",
    "        return mf_loss, emb_loss, reg_loss\n",
    "\n",
    "    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n",
    "        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "        indices = torch.from_numpy(\n",
    "            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "        values = torch.from_numpy(sparse_mx.data)\n",
    "        shape = torch.Size(sparse_mx.shape)\n",
    "        return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  # gpu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_seed(args.seed)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "\n",
    "    config = dict()\n",
    "    config['n_users'] = data_generator.n_users\n",
    "    config['n_items'] = data_generator.n_items\n",
    "\n",
    "    plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n",
    "    config['norm_adj'] = norm_adj\n",
    "\n",
    "    trainer = Trainer(data_config=config)\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55f08b-b387-46a4-95b8-71c1ae54bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\"Authorization\": \"eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOjQwMTY2LCJ1dWlkIjoiNmE1MGYzN2ItNTE1My00ZGY4LTkzZjYtZTJkNGRkZjhhMWM1IiwiaXNfYWRtaW4iOmZhbHNlLCJpc19zdXBlcl9hZG1pbiI6ZmFsc2UsInN1Yl9uYW1lIjoiIiwidGVuYW50IjoiYXV0b2RsIiwidXBrIjoiIn0.W9vekL_TuPpETo5tcNnSNn4lRLPj8znhZ7T4yFxDaKmpJIY4kLNN-RqKPHw0wZtYTZDoVE-QMlSW3Gem7Wi6Ww\"}\n",
    "resp = requests.post(\"https://www.autodl.com/api/v1/wechat/message/send\",\n",
    "                     json={\n",
    "                         \"title\": \"my_clip_rob-best-try\",\n",
    "                         \"name\": \"my_clip_rob-best-try\",\n",
    "                         \"content\": \"my_clip_rob-best-try\"\n",
    "                     }, headers = headers)\n",
    "print(resp.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b515681-9246-42ad-ba64-8f72ed59752f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed32dda-b2b2-495c-b60a-373b01d03f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0\n",
    "#{'precision': array([0.00541013, 0.00436616]), 'recall': array([0.05155236, 0.08308122]), 'ndcg': array([0.02889627, 0.03719301]), 'hit_ratio': array([0.05384418, 0.08675752]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130d871-18de-4105-9bec-57f8952d8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.001\n",
    "#{'precision': array([0.0055027 , 0.00434816]), 'recall': array([0.05240348, 0.08291837]), 'ndcg': array([0.02956245, 0.03754171]), 'hit_ratio': array([0.05476986, 0.08639753]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3f61d-45a9-4261-90e0-dd260387a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.01\n",
    "#{'precision': array([0.00540499, 0.00431473]), 'recall': array([0.05155359, 0.08194676]), 'ndcg': array([0.02885507, 0.03693886]), 'hit_ratio': array([0.05379275, 0.08578041]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ce1ce-cc34-4ab3-afd4-77f8a1f8f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.1\n",
    "#{'precision': array([0.00518385, 0.00420159]), 'recall': array([0.04933671, 0.07967111]), 'ndcg': array([0.02795288, 0.03600516]), 'hit_ratio': array([0.05163281, 0.08362047]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebf450-0aa0-4fe4-84ed-7ad57bf51313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.15\n",
    "#{'precision': array([0.00533813, 0.00422474]), 'recall': array([0.05101311, 0.0805166 ]), 'ndcg': array([0.02790774, 0.0356778 ]), 'hit_ratio': array([0.05317562, 0.0838776 ]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e4d18-291c-483d-bdb6-78d0fca3fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2\n",
    "#{'precision': array([0.00524042, 0.00420674]), 'recall': array([0.05008485, 0.07982233]), 'ndcg': array([0.02719278, 0.03508965]), 'hit_ratio': array([0.05230136, 0.08377475]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ab902-5487-4d62-80f6-4d36a0fb411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle text 0.2\n",
    "#{'precision': array([0.00533299, 0.00421445]), 'recall': array([0.0509383 , 0.08003796]), 'ndcg': array([0.0280168 , 0.03571267]), 'hit_ratio': array([0.05322705, 0.0836719 ]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f65601-8478-4059-b66a-07a553878a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle text 0.5\n",
    "#{'precision': array([0.00518385, 0.0040936 , 0.00287169]), 'recall': array([0.04942071, 0.07743403, 0.13483182]), 'ndcg': array([0.02681554, 0.03423862, 0.0462686 ]), 'hit_ratio': array([0.05168424, 0.08120339, 0.14132168]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634ebc0-22d1-4bc7-8981-e1c84e424ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle image 0.2\n",
    "#{'precision': array([0.00548727, 0.00432245, 0.0029982 ]), 'recall': array([0.05214929, 0.08205219, 0.14140825]), 'ndcg': array([0.02888834, 0.03671215, 0.04908931]), 'hit_ratio': array([0.05466701, 0.08588326, 0.14795577]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af59fce-082e-4d0b-9b4e-4d4def7bd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle image 0.5\n",
    "#{'precision': array([0.00533299, 0.00425559, 0.00303317]), 'recall': array([0.05087597, 0.08088614, 0.14285861]), 'ndcg': array([0.02836833, 0.03619914, 0.04917346]), 'hit_ratio': array([0.05317562, 0.0844433 , 0.14924145]), 'auc': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b55c7b-4934-4ad5-8905-3402dd890fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle all 0.2\n",
    "#{'precision': array([0.00527128, 0.00419131, 0.00295809]), 'recall': array([0.0503437 , 0.07946406, 0.13937394]), 'ndcg': array([0.02725028, 0.03491752, 0.04738786]), 'hit_ratio': array([0.05260993, 0.08336333, 0.14579583]), 'mrr': array([0.01985627, 0.02192239, 0.02386415]), 'auc': 0.0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
