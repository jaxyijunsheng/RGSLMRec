{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b21fa-fc4f-4626-957c-313551fed33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    class Args:\n",
    "        p=0\n",
    "        data_path = './'\n",
    "        seed = 123\n",
    "        dataset = 'baby'\n",
    "        verbose = 5\n",
    "        epoch = 2000\n",
    "        batch_size = 1024\n",
    "        regs = '[1e-5,1e-5,1e-2]'\n",
    "        lr = 0.0005\n",
    "        model_name = 'lattice'\n",
    "        embed_size = 64\n",
    "        feat_embed_dim = 64\n",
    "        weight_size = '[64,64]'\n",
    "        core = 5\n",
    "        topk = 10\n",
    "        lambda_coeff = 0.9\n",
    "        loss_ratio=0.03\n",
    "        cf_model = 'lightgcn'\n",
    "        n_layers = 1\n",
    "        layers = 1\n",
    "        sparse = 1\n",
    "        norm_type = 'sym'\n",
    "        mess_dropout = '[0.1, 0.1]'\n",
    "        early_stopping_patience = 7\n",
    "        gpu_id = 0\n",
    "        Ks = '[10, 20,50]'\n",
    "        test_flag = 'part'\n",
    "        shuffle='text'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79832fce-f477-487b-b9c3-ad0731ba2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if all_pos_num == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sum(r) / all_pos_num\n",
    "\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def mrr_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    #print(r)\n",
    "    if np.sum(r) > 0:\n",
    "        #print(1/(np.where(r==1.0)[0]+1).astype(float)[0])\n",
    "        return 1/(np.where(r==1.0)[0]+1).astype(float)[0]\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf065193-5b51-4201-9341-c04afd556689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "import json\n",
    "#from utility.parser import parse_args\n",
    "args = parse_args()\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path + '/%d-core' % args.core\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/%d-core/train.json' % (args.core)\n",
    "        val_file = path + '/%d-core/val.json' % (args.core)\n",
    "        test_file = path + '/%d-core/test.json'  % (args.core)\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        train = json.load(open(train_file))\n",
    "        test = json.load(open(test_file))\n",
    "        val = json.load(open(val_file))\n",
    "        for uid, items in train.items():\n",
    "            if len(items) == 0:\n",
    "                continue\n",
    "            uid = int(uid)\n",
    "            self.exist_users.append(uid)\n",
    "            self.n_items = max(self.n_items, max(items))\n",
    "            self.n_users = max(self.n_users, uid)\n",
    "            self.n_train += len(items)\n",
    "\n",
    "        for uid, items in test.items():\n",
    "            uid = int(uid)\n",
    "            try:\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "                self.n_users = max(self.n_users, uid)\n",
    "                self.n_test += len(items)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for uid, items in val.items():\n",
    "            uid = int(uid)\n",
    "            try:\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "                self.n_users = max(self.n_users, uid)\n",
    "                self.n_val += len(items)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n",
    "        \n",
    "        self.train_items, self.test_set, self.val_set = {}, {}, {}\n",
    "        for uid, train_items in train.items():\n",
    "            if len(train_items) == 0:\n",
    "                continue\n",
    "            uid = int(uid)\n",
    "            for idx, i in enumerate(train_items):\n",
    "                self.R[uid, i] = 1.\n",
    "\n",
    "            self.train_items[uid] = train_items\n",
    "        sp.save_npz(self.path + '/R.npz', self.R.tocsr())\n",
    "        \n",
    "        self.my_test_set=[]\n",
    "        for uid, test_items in test.items():\n",
    "            uid = int(uid)\n",
    "            if len(test_items) == 0:\n",
    "                continue\n",
    "            for i in test_items:\n",
    "                self.my_test_set.append([uid,i])\n",
    "            try:\n",
    "                self.test_set[uid] = test_items\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for uid, val_items in val.items():\n",
    "            uid = int(uid)\n",
    "            if len(val_items) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                self.val_set[uid] = val_items\n",
    "            except:\n",
    "                continue            \n",
    "\n",
    "    def get_R_mat(self):\n",
    "        t1 = time()\n",
    "        R = sp.load_npz(self.path + '/R.npz')\n",
    "        print('already load rating matrix', R.shape, time() - t1)\n",
    "        return R\n",
    "    \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def get_D_inv(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            return d_mat_inv\n",
    "\n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "\n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "            return temp\n",
    "\n",
    "        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "        # users = self.exist_users[:]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "            # neg_items += sample_neg_items_for_u(u, 3)\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4ccc8-9f4c-4647-9931-744be7acc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utility.metrics as metrics\n",
    "#from utility.parser import parse_args\n",
    "#from utility.load_data import Data\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "cores = multiprocessing.cpu_count() // 5\n",
    "\n",
    "args = parse_args()\n",
    "Ks = eval(args.Ks)\n",
    "\n",
    "data_generator = Data(path=args.data_path + args.dataset, batch_size=args.batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = metrics.auc(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio, mrr = [], [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "        mrr.append(mrr_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc, 'mrr': np.array(mrr)}\n",
    "\n",
    "\n",
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    is_val = x[-1]\n",
    "    rating = x[0]\n",
    "    #uid\n",
    "    u = x[1]\n",
    "    #user u's items in the training set\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    #user u's items in the test set\n",
    "    if is_val:\n",
    "        user_pos_test = data_generator.val_set[u]\n",
    "    else:\n",
    "        user_pos_test = data_generator.test_set[u]\n",
    "\n",
    "    all_items = set(range(ITEM_NUM))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    if args.test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "\n",
    "def test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val, drop_flag=False, batch_test_flag=False):\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)),'mrr': np.zeros(len(Ks)), 'auc': 0.}\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "        user_batch = test_users[start: end]\n",
    "        if batch_test_flag:\n",
    "            n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
    "            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "            i_count = 0\n",
    "            for i_batch_id in range(n_item_batchs):\n",
    "                i_start = i_batch_id * i_batch_size\n",
    "                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "                item_batch = range(i_start, i_end)\n",
    "                u_g_embeddings = ua_embeddings[user_batch]\n",
    "                i_g_embeddings = ia_embeddings[item_batch]\n",
    "                i_rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
    "\n",
    "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "                i_count += i_rate_batch.shape[1]\n",
    "\n",
    "            assert i_count == ITEM_NUM\n",
    "\n",
    "        else:\n",
    "            item_batch = range(ITEM_NUM)\n",
    "            u_g_embeddings = ua_embeddings[user_batch]\n",
    "            #print(max(item_batch))\n",
    "            i_g_embeddings = ia_embeddings[item_batch]\n",
    "            rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
    "\n",
    "        rate_batch = rate_batch.detach().cpu().numpy()\n",
    "        user_batch_rating_uid = zip(rate_batch, user_batch, [is_val] * len(user_batch))\n",
    "\n",
    "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision'] / n_test_users\n",
    "            result['recall'] += re['recall'] / n_test_users\n",
    "            result['ndcg'] += re['ndcg'] / n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "            result['auc'] += re['auc'] / n_test_users\n",
    "            result['mrr'] += re['mrr'] / n_test_users\n",
    "\n",
    "    assert count == n_test_users\n",
    "    pool.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53258d-b2dc-46f3-844a-0e2cf50cb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.sparse as sparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from utility.parser import parse_args\n",
    "args = parse_args()\n",
    "\n",
    "def build_knn_neighbourhood(adj, topk):\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "    return weighted_adjacency_matrix\n",
    "\n",
    "def compute_normalized_laplacian(adj):\n",
    "    if adj.shape[0]==adj.shape[1]:\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "        return L_norm\n",
    "    else:\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "        colsum = torch.sum(adj, 0)\n",
    "        d_inv_sqrt_ = torch.pow(colsum, -0.5)\n",
    "        d_inv_sqrt_[torch.isinf(d_inv_sqrt_)] = 0.\n",
    "        d_mat_inv_sqrt_ = torch.diagflat(d_inv_sqrt_)\n",
    "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt_)\n",
    "        return L_norm\n",
    "\n",
    "\n",
    "def recombine(img_feat,text_feat):\n",
    "    img_feat_norm = img_feat.div(torch.norm(img_feat, p=2, dim=-1, keepdim=True))\n",
    "    text_feat_norm = text_feat.div(torch.norm(text_feat, p=2, dim=-1, keepdim=True))\n",
    "    rel_i = F.softmax(img_feat_norm.mm(text_feat_norm.T),dim=-1)\n",
    "    rel_t = F.softmax(text_feat_norm.mm(img_feat_norm.T),dim=-1)\n",
    "    _, text_indices = torch.max(rel_i, dim=1)\n",
    "    text_sorted = text_feat[text_indices]\n",
    "    _, img_indices = torch.max(rel_t, dim=1)\n",
    "    img_sorted = img_feat[img_indices]\n",
    "    return img_sorted,text_sorted\n",
    "\n",
    "\n",
    "class reliability(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.query_0 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.key_0 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.query_1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.key_1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "    def forward(self,image_feats, text_feats):\n",
    "        image_feats_norm = image_feats.div(torch.norm(image_feats, p=2, dim=-1, keepdim=True))\n",
    "        text_feats_norm = text_feats.div(torch.norm(text_feats, p=2, dim=-1, keepdim=True))\n",
    "        rel_i = torch.matmul(self.query_0(image_feats_norm),self.key_0(text_feats_norm).T)\n",
    "        rel_t = torch.matmul(self.query_1(text_feats_norm),self.key_1(image_feats_norm).T)\n",
    "        #print(rel_i.shape)\n",
    "        diag_i = torch.diag(rel_i)\n",
    "        #print(diag_i.shape)\n",
    "        diag_t = torch.diag(rel_t)\n",
    "        return diag_i.reshape((1,-1)),diag_t.reshape((1,-1))\n",
    "\n",
    "class anomaly_encoder(nn.Module):\n",
    "    def __init__(self,adj,input_size, hidden_size,encoder_layer=3):\n",
    "        super().__init__()\n",
    "        self.encoder_layer=encoder_layer\n",
    "        self.encoder=nn.ModuleList()\n",
    "        self.encoder_=nn.ModuleList()\n",
    "        self.encoder.append(nn.Linear(input_size, hidden_size, bias=False))\n",
    "        for i in range(encoder_layer-1):\n",
    "            self.encoder.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        self.encoder_.append(nn.Linear(input_size, hidden_size, bias=False))\n",
    "        for i in range(encoder_layer-1):\n",
    "            self.encoder_.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        \n",
    "        self.ii_adj=adj\n",
    "    def forward(self,image_feats_norm, text_feats_norm):\n",
    "        #self.feat_i_tmp=image_feats_norm\n",
    "        #self.feat_t_tmp=text_feats_norm\n",
    "        feat_i_all_embeddings = [image_feats_norm]\n",
    "        feat_t_all_embeddings = [text_feats_norm]\n",
    "        for i in range(self.encoder_layer):\n",
    "            tmp_i_embeddings = F.leaky_relu(self.encoder[i](torch.sparse.mm(self.ii_adj, image_feats_norm)))\n",
    "            image_feats_norm=tmp_i_embeddings\n",
    "            feat_i_all_embeddings += [image_feats_norm]\n",
    "            tmp_t_embeddings = F.leaky_relu(self.encoder_[i](torch.sparse.mm(self.ii_adj, text_feats_norm)))\n",
    "            text_feats_norm=tmp_t_embeddings\n",
    "            feat_t_all_embeddings += [text_feats_norm]\n",
    "        feat_i_all_embeddings = torch.stack(feat_i_all_embeddings, dim=1)\n",
    "        feat_i_all_embeddings = feat_i_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        feat_t_all_embeddings = torch.stack(feat_t_all_embeddings, dim=1)\n",
    "        feat_t_all_embeddings = feat_t_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        return feat_i_all_embeddings, feat_t_all_embeddings\n",
    "\n",
    "class anomaly_decoder(nn.Module):\n",
    "    def __init__(self,adj,input_size, hidden_size,decoder_layer=3):\n",
    "        super().__init__()\n",
    "        self.decoder_layer=decoder_layer\n",
    "        self.decoder=nn.ModuleList()\n",
    "        self.decoder_=nn.ModuleList()\n",
    "        for i in range(decoder_layer-1):\n",
    "            self.decoder.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        self.decoder.append(nn.Linear(hidden_size, input_size, bias=False))\n",
    "\n",
    "        for i in range(decoder_layer-1):\n",
    "            self.decoder_.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        self.decoder_.append(nn.Linear(hidden_size, input_size, bias=False))\n",
    "        self.ii_adj=adj\n",
    "    def forward(self,image_enc, text_enc):\n",
    "        #self.feat_i_tmp=image_enc\n",
    "        #self.feat_t_tmp=text_enc\n",
    "        feat_i_all_embeddings = [image_enc]\n",
    "        feat_t_all_embeddings = [text_enc]\n",
    "        for i in range(self.decoder_layer):\n",
    "            tmp_i_embeddings = F.leaky_relu(self.decoder[i](torch.sparse.mm(self.ii_adj, image_enc)))\n",
    "            image_enc=tmp_i_embeddings\n",
    "            feat_i_all_embeddings += [image_enc]\n",
    "            tmp_t_embeddings = F.leaky_relu(self.decoder_[i](torch.sparse.mm(self.ii_adj, text_enc)))\n",
    "            text_enc=tmp_t_embeddings\n",
    "            feat_t_all_embeddings += [text_enc]\n",
    "        feat_i_all_embeddings = torch.stack(feat_i_all_embeddings, dim=1)\n",
    "        feat_i_all_embeddings = feat_i_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        feat_t_all_embeddings = torch.stack(feat_t_all_embeddings, dim=1)\n",
    "        feat_t_all_embeddings = feat_t_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        return feat_i_all_embeddings,feat_t_all_embeddings\n",
    "  \n",
    "def build_reliable_knn_neighbourhood(adj, topk,rel):\n",
    "    adj=adj.mul(F.sigmoid(rel))\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "    return weighted_adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71679f7b-c7c0-4151-805a-7890b7003ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_sim(context):\n",
    "    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n",
    "    sim = torch.mm(context_norm, context_norm.transpose(1, 0))\n",
    "    #sim.fill_diagonal_(0.)\n",
    "    return sim\n",
    "\n",
    "def build_knn_graph(adj, topk, is_sparse, norm_type):\n",
    "    adj=adj.to_dense()\n",
    "    device = adj.device\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    if is_sparse:\n",
    "        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]\n",
    "        row = [i[0] for i in tuple_list]\n",
    "        col = [i[1] for i in tuple_list]\n",
    "        i = torch.LongTensor([row, col]).to(device)\n",
    "        v = knn_val.flatten()\n",
    "        return torch.sparse_coo_tensor(i, v, adj.shape)\n",
    "    else:\n",
    "        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "        return weighted_adjacency_matrix\n",
    "\n",
    "def get_sparse_laplacian(edge_index, edge_weight, num_nodes, normalization='none'):\n",
    "    def scatter_add(src, index,dim, dim_size):\n",
    "        output = torch.zeros(dim_size, dtype=src.dtype, device=src.device)\n",
    "        return output.scatter_add_(dim=dim, index=index, src=src)\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "\n",
    "    if normalization == 'sym':\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "    elif normalization == 'rw':\n",
    "        deg_inv = 1.0 / deg\n",
    "        deg_inv.masked_fill_(deg_inv == float('inf'), 0)\n",
    "        edge_weight = deg_inv[row] * edge_weight\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "\n",
    "def get_dense_laplacian(adj, normalization='none'):\n",
    "    if normalization == 'sym':\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "    elif normalization == 'rw':\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv = torch.pow(rowsum, -1)\n",
    "        d_inv[torch.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = torch.diagflat(d_inv)\n",
    "        L_norm = torch.mm(d_mat_inv, adj)\n",
    "    elif normalization == 'none':\n",
    "        L_norm = adj\n",
    "    return L_norm\n",
    "\n",
    "def build_rel_knn_graph(adj, topk, is_sparse, norm_type,rel):\n",
    "    adj=adj.to_dense()\n",
    "    adj=adj.mul(F.sigmoid(rel))\n",
    "    device = adj.device\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    if is_sparse:\n",
    "        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]\n",
    "        row = [i[0] for i in tuple_list]\n",
    "        col = [i[1] for i in tuple_list]\n",
    "        i = torch.LongTensor([row, col]).to(device)\n",
    "        v = knn_val.flatten()\n",
    "        return torch.sparse_coo_tensor(i, v, adj.shape)\n",
    "    else:\n",
    "        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "        return weighted_adjacency_matrix\n",
    "        \n",
    "def cal_sum_lap(weight,a_sparse_list):\n",
    "    a_list=[]\n",
    "    for idx,i in enumerate(a_sparse_list):\n",
    "        a_list.append(weight[:, idx].unsqueeze(dim=1)*i.to_dense())\n",
    "    mix_a=torch.stack(a_list).sum(dim=0)\n",
    "    rowsum = torch.sum(mix_a, -1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "    colsum = torch.sum(mix_a, 0)\n",
    "    d_inv_sqrt_ = torch.pow(colsum, -0.5)\n",
    "    d_inv_sqrt_[torch.isinf(d_inv_sqrt_)] = 0.\n",
    "    d_mat_inv_sqrt_ = torch.diagflat(d_inv_sqrt_)\n",
    "    L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, mix_a), d_mat_inv_sqrt_)\n",
    "    return L_norm\n",
    "\n",
    "\n",
    "def get_dense_norm_rowandcol(adj):\n",
    "    adj=adj.to_dense()\n",
    "    rowsum = torch.sum(adj, -1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "    colsum = torch.sum(adj, 0)\n",
    "    d_inv_sqrt_ = torch.pow(colsum, -0.5)\n",
    "    d_inv_sqrt_[torch.isinf(d_inv_sqrt_)] = 0.\n",
    "    d_mat_inv_sqrt_ = torch.diagflat(d_inv_sqrt_)\n",
    "    L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt_)\n",
    "    return L_norm\n",
    "\n",
    "        \n",
    "def sparse_mat_merge(A,B):\n",
    "    A=A.coalesce()\n",
    "    B=B.coalesce()\n",
    "    values_A = A.values()\n",
    "    indices_A = A.indices()\n",
    "    values_B = B.values()\n",
    "    indices_B = B.indices()\n",
    "    new_indices_B = indices_B.clone()\n",
    "    new_indices_B[0, :] += norm_adj.shape[0]-B.shape[0]  # 行索引偏移\n",
    "    new_indices_B[1, :] += norm_adj.shape[0]-B.shape[0]  # 列索引偏移\n",
    "    mask = (indices_A[0] < norm_adj.shape[0]-B.shape[0]) | (indices_A[0] >= norm_adj.shape[0]) | \\\n",
    "           (indices_A[1] < norm_adj.shape[0]-B.shape[0]) | (indices_A[1] >= norm_adj.shape[0])\n",
    "    values_A = values_A[mask]\n",
    "    indices_A = indices_A[:, mask]\n",
    "    new_values = torch.cat([values_A, values_B], dim=0)\n",
    "    new_indices = torch.cat([indices_A, new_indices_B], dim=1)\n",
    "    new_A = torch.sparse_coo_tensor(new_indices, new_values, A.size())\n",
    "    return new_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7ae13-bbe7-401a-a2e0-f4d1bd247bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICRO(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats,rating):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weight_size = weight_size\n",
    "        self.n_ui_layers = len(self.weight_size)\n",
    "        self.weight_size = [self.embedding_dim] + self.weight_size\n",
    "        self.user_embedding = nn.Embedding(n_users, self.embedding_dim)\n",
    "        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim).to('cuda')\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n",
    "        self.rating=compute_normalized_laplacian(rating.to_dense()).to_sparse().to('cuda')\n",
    "        \n",
    "        if args.cf_model == 'ngcf':\n",
    "            self.GC_Linear_list = nn.ModuleList()\n",
    "            self.Bi_Linear_list = nn.ModuleList()\n",
    "            self.dropout_list = nn.ModuleList()\n",
    "            for i in range(self.n_ui_layers):\n",
    "                self.GC_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
    "                self.Bi_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
    "                self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
    "        \n",
    "        \n",
    "        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False).to('cuda')\n",
    "        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False).to('cuda')\n",
    "        self.image_feats_norm = self.image_embedding.weight.detach().div(torch.norm(self.image_embedding.weight.detach(), p=2, dim=-1, keepdim=True))\n",
    "        self.text_feats_norm = self.text_embedding.weight.detach().div(torch.norm(self.text_embedding.weight.detach(), p=2, dim=-1, keepdim=True))\n",
    "\n",
    "        self.image_sim = build_sim(self.image_embedding.weight.detach())\n",
    "        self.text_sim = build_sim(self.text_embedding.weight.detach())\n",
    "        \n",
    "\n",
    "        #ini_image_adj = build_sim(self.image_embedding.weight.detach()) \n",
    "        #ini_image_adj = build_knn_normalized_graph(ini_image_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "        #ini_text_adj = build_sim(self.text_embedding.weight.detach()) \n",
    "        #ini_text_adj = build_knn_normalized_graph(ini_text_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "        ii=build_knn_graph(torch.sparse.mm(self.rating.T,self.rating).to_dense(), 50, True, norm_type='sym')\n",
    "        ii=get_dense_norm_rowandcol(ii)\n",
    "        #self.an_encoder=anomaly_encoder(ini_text_adj.clone(),ini_image_adj.clone(),image_feats.shape[1],args.embed_size).to('cuda')\n",
    "        #self.an_decoder=anomaly_decoder(ini_text_adj.clone(),ini_image_adj.clone(),image_feats.shape[1],args.embed_size).to('cuda')\n",
    "        \n",
    "        self.an_encoder=anomaly_encoder(ii,image_feats.shape[1],image_feats.shape[1]).to('cuda')\n",
    "        self.an_decoder=anomaly_decoder(ii,image_feats.shape[1],image_feats.shape[1]).to('cuda')\n",
    "        \n",
    "        self.image_trs = nn.Sequential(\n",
    "            nn.Linear(4 * image_feats.shape[1], 2 * args.embed_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * self.embedding_dim, args.embed_size, bias=False)\n",
    "        )\n",
    "        self.text_trs = nn.Sequential(\n",
    "            nn.Linear(4 * text_feats.shape[1], 2 * args.embed_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * self.embedding_dim, args.embed_size, bias=False)\n",
    "        )\n",
    "\n",
    "        self.hybird_trs = nn.Linear(4*text_feats.shape[1], args.embed_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "        self.score = nn.Sequential(\n",
    "            nn.Linear(text_feats.shape[1], self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.score_ = nn.Sequential(\n",
    "            nn.Linear(image_feats.shape[1], self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.query = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.query_1 = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.dropout_list = nn.ModuleList()\n",
    "        for i in range(self.n_ui_layers):\n",
    "            self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
    "\n",
    "        self.tau = 0.5\n",
    "        self.modal_weight = nn.Parameter(torch.Tensor([[0.5, 0.5]]))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def mm(self, x, y):\n",
    "        if args.sparse:\n",
    "            return torch.sparse.mm(x, y)\n",
    "        else:\n",
    "            return torch.mm(x, y)\n",
    "    \n",
    "    def sim(self, z1, z2):\n",
    "        z1 = F.normalize(z1)\n",
    "        z2 = F.normalize(z2)\n",
    "        return torch.mm(z1, z2.t())\n",
    "\n",
    "    def batched_contrastive_loss(self, z1, z2, batch_size=256):\n",
    "        device = z1.device\n",
    "        num_nodes = z1.size(0)\n",
    "        num_batches = (num_nodes - 1) // batch_size + 1\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "        indices = torch.arange(0, num_nodes).to(device)\n",
    "        losses = []\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            mask = indices[i * batch_size:(i + 1) * batch_size]\n",
    "            refl_sim = f(self.sim(z1[mask], z1))  # [B, N]\n",
    "            between_sim = f(self.sim(z1[mask], z2))  # [B, N]\n",
    "\n",
    "            losses.append(-torch.log(\n",
    "                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n",
    "                / (refl_sim.sum(1) + between_sim.sum(1)\n",
    "                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n",
    "                   \n",
    "        loss_vec = torch.cat(losses)\n",
    "        return loss_vec.mean()\n",
    "\n",
    "    \n",
    "    def update_graph(self,image_sim,text_sim,img_rel,text_rel):\n",
    "        print('time to update graph')\n",
    "        image_adj = build_rel_knn_graph(image_sim, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type,rel=img_rel)\n",
    "        #print(self.image_a.to_dense())\n",
    "        text_adj = build_rel_knn_graph(text_sim, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type,rel=text_rel)\n",
    "        text_original_adj = text_adj.cuda()\n",
    "        image_original_adj = image_adj.cuda()\n",
    "        self.synthesis_adj=cal_sum_lap(torch.tensor([[1,1]]).cuda(),[image_adj,text_adj])#new norm\n",
    "        return get_dense_norm_rowandcol(image_original_adj),get_dense_norm_rowandcol(text_original_adj),self.synthesis_adj\n",
    "\n",
    "    def forward(self, adj, step_count,build_item_graph=False):\n",
    "        \n",
    "        img_latent,text_latent=self.an_encoder(self.image_feats_norm,self.text_feats_norm)\n",
    "        img_rec,text_rec=self.an_decoder(img_latent,text_latent)\n",
    "        \n",
    "        #text_res=text_rec-self.text_feats_norm\n",
    "        #img_res=img_rec-self.image_feats_norm\n",
    "        text_rel = self.score((text_rec-self.text_feats_norm)*(text_rec-self.text_feats_norm))\n",
    "        img_rel = self.score((img_rec-self.image_feats_norm)*(img_rec-self.image_feats_norm))\n",
    "        \n",
    "        #print('diag',diag_i.shape)\n",
    "        img_rec_loss = ((img_rec-self.image_feats_norm)*(img_rec-self.image_feats_norm)).sum(dim=-1).reshape((1,-1))\n",
    "        text_rec_loss = ((text_rec-self.text_feats_norm)*(text_rec-self.text_feats_norm)).sum(dim=-1).reshape((1,-1))\n",
    "       \n",
    "        if step_count%50==0:\n",
    "            self.image_original_adj,self.text_original_adj,synthesis_adj= self.update_graph(self.image_sim,self.text_sim,img_rel,text_rel)\n",
    "        else:\n",
    "            synthesis_adj=self.synthesis_adj.detach()\n",
    "\n",
    "        image_feats = self.image_trs(torch.cat((img_rec,self.image_embedding.weight,\n",
    "                                                self.image_embedding.weight+img_rec,self.image_embedding.weight*img_rec),dim=1))\n",
    "        text_feats = self.text_trs(torch.cat((text_rec,self.text_embedding.weight,\n",
    "                                                self.text_embedding.weight+text_rec,self.text_embedding.weight*text_rec),dim=1))\n",
    "        if step_count%200==0:\n",
    "            self.image_adj = build_sim(image_feats) \n",
    "            self.image_adj = build_knn_graph(self.image_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "            self.image_adj = get_dense_norm_rowandcol(self.image_adj)\n",
    "            self.image_adj = (1 - args.lambda_coeff) * self.image_adj + args.lambda_coeff * self.image_original_adj\n",
    "            self.text_adj = build_sim(text_feats) \n",
    "            self.text_adj = build_knn_graph(self.text_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "            self.text_adj = get_dense_norm_rowandcol(self.text_adj)\n",
    "            self.text_adj = (1 - args.lambda_coeff) * self.text_adj + args.lambda_coeff * self.text_original_adj\n",
    "        else:\n",
    "            self.image_adj = self.image_adj.detach()\n",
    "            self.text_adj = self.text_adj.detach()\n",
    "        image_item_embeds = self.item_id_embedding.weight\n",
    "        text_item_embeds = self.item_id_embedding.weight\n",
    "\n",
    "        for i in range(args.layers):\n",
    "            image_item_embeds = self.mm(self.image_adj, image_item_embeds)\n",
    "            text_item_embeds = self.mm(self.text_adj, text_item_embeds)\n",
    "\n",
    "        att = torch.cat([self.query(image_item_embeds), self.query(text_item_embeds)], dim=-1)\n",
    "        weight = self.softmax(att)\n",
    "        h = weight[:, 0].unsqueeze(dim=1) * (image_item_embeds) + weight[:,1].unsqueeze(dim=1) * (text_item_embeds)\n",
    "        \n",
    "        uii_i_emb=torch.sparse.mm(self.rating, image_item_embeds)\n",
    "        uii_t_emb=torch.sparse.mm(self.rating, text_item_embeds)\n",
    "        image_user_feats=torch.sparse.mm(self.rating, image_feats)\n",
    "        text_user_feats=torch.sparse.mm(self.rating, text_feats)\n",
    "        \n",
    "        att_ = torch.cat([self.query_1(uii_i_emb), self.query_1(uii_t_emb)], dim=-1)\n",
    "        modal_weight = self.softmax(att_)\n",
    "        hh=modal_weight[:, 0].unsqueeze(dim=1) * uii_i_emb + modal_weight[:, 1].unsqueeze(dim=1) * uii_t_emb\n",
    "        \n",
    "        ego_embeddings_u=self.user_embedding.weight\n",
    "        ego_embeddings_i=self.item_id_embedding.weight\n",
    "        user_embeddings=[self.user_embedding.weight]\n",
    "        item_embeddings=[self.item_id_embedding.weight]\n",
    "        \n",
    "        for i in range(self.n_ui_layers):\n",
    "            side_embeddings_u = torch.sparse.mm(self.rating, ego_embeddings_i)\n",
    "            side_embeddings_u=self.dropout_list[i](side_embeddings_u)\n",
    "            \n",
    "\n",
    "            side_embeddings_i = torch.sparse.mm(self.rating.T, ego_embeddings_u) + torch.sparse.mm(synthesis_adj, ego_embeddings_i)\n",
    "            side_embeddings_i=self.dropout_list[i](side_embeddings_i)\n",
    "            \n",
    "            ego_embeddings_u = side_embeddings_u\n",
    "            user_embeddings += [ego_embeddings_u]\n",
    "            ego_embeddings_i = side_embeddings_i\n",
    "            item_embeddings += [ego_embeddings_i]\n",
    "        user_embeddings = torch.stack(user_embeddings, dim=1)\n",
    "        user_embeddings = user_embeddings.mean(dim=1, keepdim=False)\n",
    "        \n",
    "        item_embeddings = torch.stack(item_embeddings, dim=1)\n",
    "        item_embeddings = item_embeddings.mean(dim=1, keepdim=False)\n",
    "        \n",
    "        i_g_embeddings = item_embeddings\n",
    "        u_g_embeddings = user_embeddings\n",
    "        i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n",
    "        u_g_embeddings = u_g_embeddings + F.normalize(hh, p=2, dim=1)\n",
    "        return u_g_embeddings, i_g_embeddings, image_item_embeds, text_item_embeds, h, img_rec_loss,text_rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683a438-f619-4919-93df-25e6106b3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time as tm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.sparse as sparse\n",
    "\n",
    "#from utility.parser import parse_args\n",
    "#from Models import LATTICE\n",
    "#from utility.batch_test import *\n",
    "\n",
    "args = parse_args()\n",
    "rec_epoch=10\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, data_config):\n",
    "        # argument settings\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "\n",
    "        self.model_name = args.model_name\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.lr = args.lr\n",
    "        self.emb_dim = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.weight_size = eval(args.weight_size)\n",
    "        self.n_layers = len(self.weight_size)\n",
    "        self.regs = eval(args.regs)\n",
    "        self.decay = self.regs[0]\n",
    "\n",
    "        self.norm_adj = data_config['norm_adj']\n",
    "        self.norm_adj = self.sparse_mx_to_torch_sparse_tensor(self.norm_adj).float().cuda()\n",
    "        self.rating=data_config['rating']\n",
    "        self.rating = self.sparse_mx_to_torch_sparse_tensor(self.rating).float().cuda()\n",
    "\n",
    "        '''\n",
    "        if args.shuffle=='all':\n",
    "            image_feats = np.load('./{}/clip_trm_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/clip_trm_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_trm_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_trm_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='text':\n",
    "            image_feats = np.load('./{}/clip_trm_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/clip_trm_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_trm_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_trm_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='image':\n",
    "            image_feats = np.load('./{}/clip_trm_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/clip_trm_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_trm_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_trm_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        else:\n",
    "            image_feats = np.load('./{}/clip_trm_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/clip_trm_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_trm_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_trm_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        '''\n",
    "        if args.shuffle=='all':\n",
    "            image_feats = np.load('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='text':\n",
    "            image_feats = np.load('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='image':\n",
    "            image_feats = np.load('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        else:\n",
    "            image_feats = np.load('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print('./{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        self.model = MICRO(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, image_feats, text_feats, self.rating)\n",
    "        self.model = self.model.cuda()\n",
    "        \n",
    "        self.rec_optimizer = optim.Adam([{'params': self.model.an_encoder.parameters()},{'params': self.model.an_decoder.parameters()}], lr=0.01)\n",
    "        \n",
    "\n",
    "    def set_lr_scheduler(self):\n",
    "        fac = lambda epoch: 0.96 ** (epoch / 50)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)\n",
    "        return scheduler\n",
    "    def forzen_ano_train_other(self):\n",
    "        for param in self.model.an_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.an_decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def test(self, users_to_test, is_val):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            ua_embeddings, ia_embeddings, *rest = self.model(self.norm_adj,0, build_item_graph=True)\n",
    "        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n",
    "        return result\n",
    "\n",
    "    def train(self):\n",
    "        training_time_list = []\n",
    "        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger,mrr_loger = [], [], [], [], [], []\n",
    "        stopping_step = 0\n",
    "        should_stop = False\n",
    "        cur_best_pre_0 = 0.\n",
    "\n",
    "        n_batch = data_generator.n_train // args.batch_size + 1\n",
    "        best_recall = 0\n",
    "        for epoch in (range(args.epoch)):\n",
    "            if epoch==rec_epoch:\n",
    "                self.optimizer=self.forzen_ano_train_other()\n",
    "                self.lr_scheduler = self.set_lr_scheduler()\n",
    "            t1 = time()\n",
    "            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
    "            l_rec_loss= 0. \n",
    "            contrastive_loss = 0.\n",
    "            n_batch = data_generator.n_train // args.batch_size + 1\n",
    "            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.\n",
    "            sample_time = 0.\n",
    "            build_item_graph = True\n",
    "            for idx in (range(n_batch)):\n",
    "                #print('batch num ',idx)\n",
    "                self.model.train()\n",
    "                torch.autograd.set_detect_anomaly(False)\n",
    "                if epoch>=rec_epoch:\n",
    "                    self.optimizer.zero_grad()\n",
    "                self.rec_optimizer.zero_grad()\n",
    "                sample_t1 = time()\n",
    "                users, pos_items, neg_items = data_generator.sample()\n",
    "                sample_time += time() - sample_t1\n",
    "                #if idx%50==0:\n",
    "                ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, fusion_embed, img_rel,text_rel = self.model(self.norm_adj,idx, build_item_graph=build_item_graph)\n",
    "                #else:\n",
    "                    #ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, fusion_embed = self.model(self.norm_adj,idx, build_item_graph=build_item_graph)\n",
    "                #build_item_graph = False\n",
    "                u_g_embeddings = ua_embeddings[users]\n",
    "                pos_i_g_embeddings = ia_embeddings[pos_items]\n",
    "                neg_i_g_embeddings = ia_embeddings[neg_items]\n",
    "\n",
    "\n",
    "                batch_mf_loss, batch_emb_loss, batch_reg_loss = self.bpr_loss(u_g_embeddings, pos_i_g_embeddings,\n",
    "                                                                              neg_i_g_embeddings)\n",
    "\n",
    "                \n",
    "                \n",
    "                batch_contrastive_loss = 0\n",
    "                batch_contrastive_loss += self.model.batched_contrastive_loss(image_item_embeds,fusion_embed)\n",
    "                batch_contrastive_loss += self.model.batched_contrastive_loss(text_item_embeds,fusion_embed)\n",
    "\n",
    "                #batch_contrastive_loss += self.model.batched_contrastive_loss(uii_i_emb,hh)\n",
    "                #batch_contrastive_loss += self.model.batched_contrastive_loss(uii_t_emb,hh)\n",
    "\n",
    "                batch_l_rec_loss=(img_rel+text_rel).mean()\n",
    "                batch_l_rec_loss*=0.1\n",
    "                batch_contrastive_loss *=  args.loss_ratio\n",
    "                batch_loss = batch_mf_loss + batch_emb_loss + batch_reg_loss + batch_contrastive_loss + batch_l_rec_loss\n",
    "                if epoch<rec_epoch:\n",
    "                    batch_l_rec_loss.backward(retain_graph=False)\n",
    "                    self.rec_optimizer.step()\n",
    "                else:\n",
    "                    \n",
    "                    batch_loss.backward(retain_graph=False)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                loss += float(batch_loss)\n",
    "                mf_loss += float(batch_mf_loss)\n",
    "                emb_loss += float(batch_emb_loss)\n",
    "                reg_loss += float(batch_reg_loss)\n",
    "                contrastive_loss += float(batch_contrastive_loss)\n",
    "                l_rec_loss +=batch_l_rec_loss\n",
    "                #l_rec_loss+=0\n",
    "            if epoch>=rec_epoch:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            del ua_embeddings, ia_embeddings, u_g_embeddings, neg_i_g_embeddings, pos_i_g_embeddings\n",
    "\n",
    "            if math.isnan(loss) == True:\n",
    "                print('ERROR: loss is nan.')\n",
    "                sys.exit()\n",
    "\n",
    "            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f + %.5f + %.5f + %.5f]' % (\n",
    "                epoch, time() - t1, loss, mf_loss, emb_loss, reg_loss, contrastive_loss, l_rec_loss)\n",
    "            training_time_list.append(time() - t1)\n",
    "            print(perf_str)\n",
    "\n",
    "            if epoch<rec_epoch+5:\n",
    "                continue\n",
    "            \n",
    "            if epoch % args.verbose != 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            t2 = time()\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            users_to_val = list(data_generator.val_set.keys())\n",
    "            ret = self.test(users_to_val, is_val=True)\n",
    "            training_time_list.append(t2 - t1)\n",
    "\n",
    "            t3 = time()\n",
    "\n",
    "            loss_loger.append(loss)\n",
    "            rec_loger.append(ret['recall'])\n",
    "            pre_loger.append(ret['precision'])\n",
    "            ndcg_loger.append(ret['ndcg'])\n",
    "            hit_loger.append(ret['hit_ratio'])\n",
    "            if args.verbose > 0:\n",
    "                perf_str = 'Epoch %d [%.1fs + %.1fs]:  val==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
    "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], mrr=[%.5f, %.5f]' % \\\n",
    "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0],\n",
    "                            ret['recall'][1],\n",
    "                            ret['precision'][0], ret['precision'][1], ret['hit_ratio'][0], ret['hit_ratio'][1],\n",
    "                            ret['ndcg'][0], ret['ndcg'][1],ret['mrr'][0], ret['mrr'][1])\n",
    "                print(perf_str)\n",
    "\n",
    "            if ret['recall'][1] > best_recall:\n",
    "                best_recall = ret['recall'][1]\n",
    "                test_ret = self.test(users_to_test, is_val=False)\n",
    "                perf_str = 'Epoch %d [%.1fs + %.1fs]: test==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
    "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], mrr=[%.5f, %.5f]' % \\\n",
    "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, test_ret['recall'][0],\n",
    "                            test_ret['recall'][1],\n",
    "                            test_ret['precision'][0], test_ret['precision'][1], test_ret['hit_ratio'][0], test_ret['hit_ratio'][1],\n",
    "                            test_ret['ndcg'][0], test_ret['ndcg'][1],test_ret['mrr'][0], test_ret['mrr'][1])\n",
    "                print(perf_str)                \n",
    "                stopping_step = 0\n",
    "            elif stopping_step < args.early_stopping_patience:\n",
    "                stopping_step += 1\n",
    "                print('#####Early stopping steps: %d #####' % stopping_step)\n",
    "            else:\n",
    "                print('#####Early stop! #####')\n",
    "                break\n",
    "        print(test_ret)\n",
    "        print(args.dataset,args.shuffle,args.p,tm.strftime(\"%a %b %d %H:%M:%S %Y\", tm.localtime()),test_ret, file=open('test_ret.txt','a'))\n",
    "        \n",
    "\n",
    "    def bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n",
    "\n",
    "        regularizer = 1./2*(users**2).sum() + 1./2*(pos_items**2).sum() + 1./2*(neg_items**2).sum()\n",
    "        regularizer = regularizer / self.batch_size\n",
    "\n",
    "        maxi = F.logsigmoid(pos_scores - neg_scores)\n",
    "        mf_loss = -torch.mean(maxi)\n",
    "\n",
    "        emb_loss = self.decay * regularizer\n",
    "        reg_loss = 0.0\n",
    "        return mf_loss, emb_loss, reg_loss\n",
    "\n",
    "    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n",
    "        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "        indices = torch.from_numpy(\n",
    "            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "        values = torch.from_numpy(sparse_mx.data)\n",
    "        shape = torch.Size(sparse_mx.shape)\n",
    "        return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  # gpu\n",
    "\n",
    "\n",
    "set_seed(args.seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "\n",
    "config = dict()\n",
    "config['n_users'] = data_generator.n_users\n",
    "config['n_items'] = data_generator.n_items\n",
    "\n",
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n",
    "\n",
    "rating=data_generator.get_R_mat()\n",
    "\n",
    "config['norm_adj'] = norm_adj\n",
    "config['rating'] = rating\n",
    "\n",
    "trainer = Trainer(data_config=config)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22c5d6-0b71-4cc9-9c9a-d53272e016e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\"Authorization\": \"eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOjQwMTY2LCJ1dWlkIjoiNmE1MGYzN2ItNTE1My00ZGY4LTkzZjYtZTJkNGRkZjhhMWM1IiwiaXNfYWRtaW4iOmZhbHNlLCJpc19zdXBlcl9hZG1pbiI6ZmFsc2UsInN1Yl9uYW1lIjoiIiwidGVuYW50IjoiYXV0b2RsIiwidXBrIjoiIn0.W9vekL_TuPpETo5tcNnSNn4lRLPj8znhZ7T4yFxDaKmpJIY4kLNN-RqKPHw0wZtYTZDoVE-QMlSW3Gem7Wi6Ww\"}\n",
    "resp = requests.post(\"https://www.autodl.com/api/v1/wechat/message/send\",\n",
    "                     json={\n",
    "                         \"title\": \"my_clip_rob-best-try\",\n",
    "                         \"name\": \"my_clip_rob-best-try\",\n",
    "                         \"content\": \"my_clip_rob-best-try\"\n",
    "                     }, headers = headers)\n",
    "print(resp.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888393f-fd31-4d7b-be4d-aa8304448753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddc84b-499f-414b-85eb-4705b5cc0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95907d9d-a499-403f-bf02-1fa47483c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd1566-1e1a-46eb-b797-be6f92ad9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d9179-173c-4420-89ff-5331ce52db3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
