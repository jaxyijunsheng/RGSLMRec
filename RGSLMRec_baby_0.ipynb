{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195b21fa-fc4f-4626-957c-313551fed33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    class Args:\n",
    "        p=0\n",
    "        data_path = '../'\n",
    "        seed = 123\n",
    "        dataset = 'baby'\n",
    "        verbose = 5\n",
    "        epoch = 2000\n",
    "        batch_size = 1024\n",
    "        regs = '[1e-5,1e-5,1e-2]'\n",
    "        lr = 0.0005\n",
    "        model_name = 'lattice'\n",
    "        embed_size = 64\n",
    "        feat_embed_dim = 64\n",
    "        weight_size = '[64,64]'\n",
    "        core = 5\n",
    "        topk = 10\n",
    "        lambda_coeff = 0.9\n",
    "        loss_ratio=0.03\n",
    "        cf_model = 'lightgcn'\n",
    "        n_layers = 1\n",
    "        layers = 1\n",
    "        sparse = 1\n",
    "        norm_type = 'sym'\n",
    "        mess_dropout = '[0.1, 0.1]'\n",
    "        early_stopping_patience = 7\n",
    "        gpu_id = 0\n",
    "        Ks = '[10, 20,50]'\n",
    "        test_flag = 'part'\n",
    "        shuffle='text'\n",
    "    return Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79832fce-f477-487b-b9c3-ad0731ba2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if all_pos_num == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sum(r) / all_pos_num\n",
    "\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def mrr_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    #print(r)\n",
    "    if np.sum(r) > 0:\n",
    "        #print(1/(np.where(r==1.0)[0]+1).astype(float)[0])\n",
    "        return 1/(np.where(r==1.0)[0]+1).astype(float)[0]\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf065193-5b51-4201-9341-c04afd556689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "import json\n",
    "#from utility.parser import parse_args\n",
    "args = parse_args()\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path + '/%d-core' % args.core\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/%d-core/train.json' % (args.core)\n",
    "        val_file = path + '/%d-core/val.json' % (args.core)\n",
    "        test_file = path + '/%d-core/test.json'  % (args.core)\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        train = json.load(open(train_file))\n",
    "        test = json.load(open(test_file))\n",
    "        val = json.load(open(val_file))\n",
    "        for uid, items in train.items():\n",
    "            if len(items) == 0:\n",
    "                continue\n",
    "            uid = int(uid)\n",
    "            self.exist_users.append(uid)\n",
    "            self.n_items = max(self.n_items, max(items))\n",
    "            self.n_users = max(self.n_users, uid)\n",
    "            self.n_train += len(items)\n",
    "\n",
    "        for uid, items in test.items():\n",
    "            uid = int(uid)\n",
    "            try:\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "                self.n_users = max(self.n_users, uid)\n",
    "                self.n_test += len(items)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for uid, items in val.items():\n",
    "            uid = int(uid)\n",
    "            try:\n",
    "                self.n_items = max(self.n_items, max(items))\n",
    "                self.n_users = max(self.n_users, uid)\n",
    "                self.n_val += len(items)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n",
    "        \n",
    "        self.train_items, self.test_set, self.val_set = {}, {}, {}\n",
    "        for uid, train_items in train.items():\n",
    "            if len(train_items) == 0:\n",
    "                continue\n",
    "            uid = int(uid)\n",
    "            for idx, i in enumerate(train_items):\n",
    "                self.R[uid, i] = 1.\n",
    "\n",
    "            self.train_items[uid] = train_items\n",
    "        sp.save_npz(self.path + '/R.npz', self.R.tocsr())\n",
    "        \n",
    "        self.my_test_set=[]\n",
    "        for uid, test_items in test.items():\n",
    "            uid = int(uid)\n",
    "            if len(test_items) == 0:\n",
    "                continue\n",
    "            for i in test_items:\n",
    "                self.my_test_set.append([uid,i])\n",
    "            try:\n",
    "                self.test_set[uid] = test_items\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for uid, val_items in val.items():\n",
    "            uid = int(uid)\n",
    "            if len(val_items) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                self.val_set[uid] = val_items\n",
    "            except:\n",
    "                continue            \n",
    "\n",
    "    def get_R_mat(self):\n",
    "        t1 = time()\n",
    "        R = sp.load_npz(self.path + '/R.npz')\n",
    "        print('already load rating matrix', R.shape, time() - t1)\n",
    "        return R\n",
    "    \n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def get_D_inv(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "            return d_mat_inv\n",
    "\n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "\n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "            return temp\n",
    "\n",
    "        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "        # users = self.exist_users[:]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "            # neg_items += sample_neg_items_for_u(u, 3)\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d4ccc8-9f4c-4647-9931-744be7acc558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=19445, n_items=7050\n",
      "n_interactions=139110\n",
      "n_train=118551, n_test=20559, sparsity=0.00101\n"
     ]
    }
   ],
   "source": [
    "#import utility.metrics as metrics\n",
    "#from utility.parser import parse_args\n",
    "#from utility.load_data import Data\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "cores = multiprocessing.cpu_count() // 5\n",
    "\n",
    "args = parse_args()\n",
    "Ks = eval(args.Ks)\n",
    "\n",
    "data_generator = Data(path=args.data_path + args.dataset, batch_size=args.batch_size)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = metrics.auc(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio, mrr = [], [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(precision_at_k(r, K))\n",
    "        recall.append(recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(ndcg_at_k(r, K))\n",
    "        hit_ratio.append(hit_at_k(r, K))\n",
    "        mrr.append(mrr_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc, 'mrr': np.array(mrr)}\n",
    "\n",
    "\n",
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    is_val = x[-1]\n",
    "    rating = x[0]\n",
    "    #uid\n",
    "    u = x[1]\n",
    "    #user u's items in the training set\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    #user u's items in the test set\n",
    "    if is_val:\n",
    "        user_pos_test = data_generator.val_set[u]\n",
    "    else:\n",
    "        user_pos_test = data_generator.test_set[u]\n",
    "\n",
    "    all_items = set(range(ITEM_NUM))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "    #test_items = list(all_items)\n",
    "    if args.test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "\n",
    "def test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val, drop_flag=False, batch_test_flag=False):\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)),'mrr': np.zeros(len(Ks)), 'auc': 0.}\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "        user_batch = test_users[start: end]\n",
    "        if batch_test_flag:\n",
    "            n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
    "            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "            i_count = 0\n",
    "            for i_batch_id in range(n_item_batchs):\n",
    "                i_start = i_batch_id * i_batch_size\n",
    "                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "                item_batch = range(i_start, i_end)\n",
    "                u_g_embeddings = ua_embeddings[user_batch]\n",
    "                i_g_embeddings = ia_embeddings[item_batch]\n",
    "                i_rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
    "\n",
    "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "                i_count += i_rate_batch.shape[1]\n",
    "\n",
    "            assert i_count == ITEM_NUM\n",
    "\n",
    "        else:\n",
    "            item_batch = range(ITEM_NUM)\n",
    "            u_g_embeddings = ua_embeddings[user_batch]\n",
    "            #print(max(item_batch))\n",
    "            i_g_embeddings = ia_embeddings[item_batch]\n",
    "            rate_batch = torch.matmul(u_g_embeddings, torch.transpose(i_g_embeddings, 0, 1))\n",
    "\n",
    "        rate_batch = rate_batch.detach().cpu().numpy()\n",
    "        user_batch_rating_uid = zip(rate_batch, user_batch, [is_val] * len(user_batch))\n",
    "\n",
    "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision'] / n_test_users\n",
    "            result['recall'] += re['recall'] / n_test_users\n",
    "            result['ndcg'] += re['ndcg'] / n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "            result['auc'] += re['auc'] / n_test_users\n",
    "            result['mrr'] += re['mrr'] / n_test_users\n",
    "\n",
    "    assert count == n_test_users\n",
    "    pool.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b53258d-b2dc-46f3-844a-0e2cf50cb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.sparse as sparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from utility.parser import parse_args\n",
    "args = parse_args()\n",
    "\n",
    "def build_knn_neighbourhood(adj, topk):\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "    return weighted_adjacency_matrix\n",
    "\n",
    "def compute_normalized_laplacian(adj):\n",
    "    if adj.shape[0]==adj.shape[1]:\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "        return L_norm\n",
    "    else:\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "        colsum = torch.sum(adj, 0)\n",
    "        d_inv_sqrt_ = torch.pow(colsum, -0.5)\n",
    "        d_inv_sqrt_[torch.isinf(d_inv_sqrt_)] = 0.\n",
    "        d_mat_inv_sqrt_ = torch.diagflat(d_inv_sqrt_)\n",
    "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt_)\n",
    "        return L_norm\n",
    "\n",
    "\n",
    "def recombine(img_feat,text_feat):\n",
    "    img_feat_norm = img_feat.div(torch.norm(img_feat, p=2, dim=-1, keepdim=True))\n",
    "    text_feat_norm = text_feat.div(torch.norm(text_feat, p=2, dim=-1, keepdim=True))\n",
    "    rel_i = F.softmax(img_feat_norm.mm(text_feat_norm.T),dim=-1)\n",
    "    rel_t = F.softmax(text_feat_norm.mm(img_feat_norm.T),dim=-1)\n",
    "    _, text_indices = torch.max(rel_i, dim=1)\n",
    "    text_sorted = text_feat[text_indices]\n",
    "    _, img_indices = torch.max(rel_t, dim=1)\n",
    "    img_sorted = img_feat[img_indices]\n",
    "    return img_sorted,text_sorted\n",
    "\n",
    "\n",
    "class reliability(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.query_0 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.key_0 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.query_1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.key_1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "    def forward(self,image_feats, text_feats):\n",
    "        image_feats_norm = image_feats.div(torch.norm(image_feats, p=2, dim=-1, keepdim=True))\n",
    "        text_feats_norm = text_feats.div(torch.norm(text_feats, p=2, dim=-1, keepdim=True))\n",
    "        rel_i = torch.matmul(self.query_0(image_feats_norm),self.key_0(text_feats_norm).T)\n",
    "        rel_t = torch.matmul(self.query_1(text_feats_norm),self.key_1(image_feats_norm).T)\n",
    "        #print(rel_i.shape)\n",
    "        diag_i = torch.diag(rel_i)\n",
    "        #print(diag_i.shape)\n",
    "        diag_t = torch.diag(rel_t)\n",
    "        return diag_i.reshape((1,-1)),diag_t.reshape((1,-1))\n",
    "\n",
    "class anomaly_encoder(nn.Module):\n",
    "    def __init__(self,adj,input_size, hidden_size,encoder_layer=3):\n",
    "        super().__init__()\n",
    "        self.encoder_layer=encoder_layer\n",
    "        self.encoder=nn.ModuleList()\n",
    "        self.encoder_=nn.ModuleList()\n",
    "        self.encoder.append(nn.Linear(input_size, hidden_size, bias=False))\n",
    "        for i in range(encoder_layer-1):\n",
    "            self.encoder.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        self.encoder_.append(nn.Linear(input_size, hidden_size, bias=False))\n",
    "        for i in range(encoder_layer-1):\n",
    "            self.encoder_.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        \n",
    "        self.ii_adj=adj\n",
    "    def forward(self,image_feats_norm, text_feats_norm):\n",
    "        #self.feat_i_tmp=image_feats_norm\n",
    "        #self.feat_t_tmp=text_feats_norm\n",
    "        feat_i_all_embeddings = [image_feats_norm]\n",
    "        feat_t_all_embeddings = [text_feats_norm]\n",
    "        for i in range(self.encoder_layer):\n",
    "            tmp_i_embeddings = F.leaky_relu(self.encoder[i](torch.sparse.mm(self.ii_adj, image_feats_norm)))\n",
    "            image_feats_norm=tmp_i_embeddings\n",
    "            feat_i_all_embeddings += [image_feats_norm]\n",
    "            tmp_t_embeddings = F.leaky_relu(self.encoder_[i](torch.sparse.mm(self.ii_adj, text_feats_norm)))\n",
    "            text_feats_norm=tmp_t_embeddings\n",
    "            feat_t_all_embeddings += [text_feats_norm]\n",
    "        feat_i_all_embeddings = torch.stack(feat_i_all_embeddings, dim=1)\n",
    "        feat_i_all_embeddings = feat_i_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        feat_t_all_embeddings = torch.stack(feat_t_all_embeddings, dim=1)\n",
    "        feat_t_all_embeddings = feat_t_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        return feat_i_all_embeddings, feat_t_all_embeddings\n",
    "\n",
    "class anomaly_decoder(nn.Module):\n",
    "    def __init__(self,adj,input_size, hidden_size,decoder_layer=3):\n",
    "        super().__init__()\n",
    "        self.decoder_layer=decoder_layer\n",
    "        self.decoder=nn.ModuleList()\n",
    "        self.decoder_=nn.ModuleList()\n",
    "        for i in range(decoder_layer-1):\n",
    "            self.decoder.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        self.decoder.append(nn.Linear(hidden_size, input_size, bias=False))\n",
    "\n",
    "        for i in range(decoder_layer-1):\n",
    "            self.decoder_.append(nn.Linear(hidden_size, hidden_size, bias=False))\n",
    "        self.decoder_.append(nn.Linear(hidden_size, input_size, bias=False))\n",
    "        self.ii_adj=adj\n",
    "    def forward(self,image_enc, text_enc):\n",
    "        #self.feat_i_tmp=image_enc\n",
    "        #self.feat_t_tmp=text_enc\n",
    "        feat_i_all_embeddings = [image_enc]\n",
    "        feat_t_all_embeddings = [text_enc]\n",
    "        for i in range(self.decoder_layer):\n",
    "            tmp_i_embeddings = F.leaky_relu(self.decoder[i](torch.sparse.mm(self.ii_adj, image_enc)))\n",
    "            image_enc=tmp_i_embeddings\n",
    "            feat_i_all_embeddings += [image_enc]\n",
    "            tmp_t_embeddings = F.leaky_relu(self.decoder_[i](torch.sparse.mm(self.ii_adj, text_enc)))\n",
    "            text_enc=tmp_t_embeddings\n",
    "            feat_t_all_embeddings += [text_enc]\n",
    "        feat_i_all_embeddings = torch.stack(feat_i_all_embeddings, dim=1)\n",
    "        feat_i_all_embeddings = feat_i_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        feat_t_all_embeddings = torch.stack(feat_t_all_embeddings, dim=1)\n",
    "        feat_t_all_embeddings = feat_t_all_embeddings.mean(dim=1, keepdim=False)\n",
    "        return feat_i_all_embeddings,feat_t_all_embeddings\n",
    "  \n",
    "def build_reliable_knn_neighbourhood(adj, topk,rel):\n",
    "    adj=adj.mul(F.sigmoid(rel))\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "    return weighted_adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71679f7b-c7c0-4151-805a-7890b7003ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_sim(context):\n",
    "    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n",
    "    sim = torch.mm(context_norm, context_norm.transpose(1, 0))\n",
    "    #sim.fill_diagonal_(0.)\n",
    "    return sim\n",
    "\n",
    "def build_knn_graph(adj, topk, is_sparse, norm_type):\n",
    "    adj=adj.to_dense()\n",
    "    device = adj.device\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    if is_sparse:\n",
    "        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]\n",
    "        row = [i[0] for i in tuple_list]\n",
    "        col = [i[1] for i in tuple_list]\n",
    "        i = torch.LongTensor([row, col]).to(device)\n",
    "        v = knn_val.flatten()\n",
    "        return torch.sparse_coo_tensor(i, v, adj.shape)\n",
    "    else:\n",
    "        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "        return weighted_adjacency_matrix\n",
    "\n",
    "def get_sparse_laplacian(edge_index, edge_weight, num_nodes, normalization='none'):\n",
    "    def scatter_add(src, index,dim, dim_size):\n",
    "        output = torch.zeros(dim_size, dtype=src.dtype, device=src.device)\n",
    "        return output.scatter_add_(dim=dim, index=index, src=src)\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "\n",
    "    if normalization == 'sym':\n",
    "        deg_inv_sqrt = deg.pow_(-0.5)\n",
    "        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "    elif normalization == 'rw':\n",
    "        deg_inv = 1.0 / deg\n",
    "        deg_inv.masked_fill_(deg_inv == float('inf'), 0)\n",
    "        edge_weight = deg_inv[row] * edge_weight\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "\n",
    "def get_dense_laplacian(adj, normalization='none'):\n",
    "    if normalization == 'sym':\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "    elif normalization == 'rw':\n",
    "        rowsum = torch.sum(adj, -1)\n",
    "        d_inv = torch.pow(rowsum, -1)\n",
    "        d_inv[torch.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = torch.diagflat(d_inv)\n",
    "        L_norm = torch.mm(d_mat_inv, adj)\n",
    "    elif normalization == 'none':\n",
    "        L_norm = adj\n",
    "    return L_norm\n",
    "\n",
    "def build_rel_knn_graph(adj, topk, is_sparse, norm_type,rel):\n",
    "    adj=adj.to_dense()\n",
    "    adj=adj.mul(F.sigmoid(rel))\n",
    "    device = adj.device\n",
    "    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n",
    "    if is_sparse:\n",
    "        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]\n",
    "        row = [i[0] for i in tuple_list]\n",
    "        col = [i[1] for i in tuple_list]\n",
    "        i = torch.LongTensor([row, col]).to(device)\n",
    "        v = knn_val.flatten()\n",
    "        return torch.sparse_coo_tensor(i, v, adj.shape)\n",
    "    else:\n",
    "        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n",
    "        return weighted_adjacency_matrix\n",
    "        \n",
    "def cal_sum_lap(weight,a_sparse_list):\n",
    "    a_list=[]\n",
    "    for idx,i in enumerate(a_sparse_list):\n",
    "        a_list.append(weight[:, idx].unsqueeze(dim=1)*i.to_dense())\n",
    "    mix_a=torch.stack(a_list).sum(dim=0)\n",
    "    rowsum = torch.sum(mix_a, -1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "    colsum = torch.sum(mix_a, 0)\n",
    "    d_inv_sqrt_ = torch.pow(colsum, -0.5)\n",
    "    d_inv_sqrt_[torch.isinf(d_inv_sqrt_)] = 0.\n",
    "    d_mat_inv_sqrt_ = torch.diagflat(d_inv_sqrt_)\n",
    "    L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, mix_a), d_mat_inv_sqrt_)\n",
    "    return L_norm\n",
    "\n",
    "\n",
    "def get_dense_norm_rowandcol(adj):\n",
    "    adj=adj.to_dense()\n",
    "    rowsum = torch.sum(adj, -1)\n",
    "    d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n",
    "    colsum = torch.sum(adj, 0)\n",
    "    d_inv_sqrt_ = torch.pow(colsum, -0.5)\n",
    "    d_inv_sqrt_[torch.isinf(d_inv_sqrt_)] = 0.\n",
    "    d_mat_inv_sqrt_ = torch.diagflat(d_inv_sqrt_)\n",
    "    L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt_)\n",
    "    return L_norm\n",
    "\n",
    "        \n",
    "def sparse_mat_merge(A,B):\n",
    "    A=A.coalesce()\n",
    "    B=B.coalesce()\n",
    "    values_A = A.values()\n",
    "    indices_A = A.indices()\n",
    "    values_B = B.values()\n",
    "    indices_B = B.indices()\n",
    "    new_indices_B = indices_B.clone()\n",
    "    new_indices_B[0, :] += norm_adj.shape[0]-B.shape[0]  # 行索引偏移\n",
    "    new_indices_B[1, :] += norm_adj.shape[0]-B.shape[0]  # 列索引偏移\n",
    "    mask = (indices_A[0] < norm_adj.shape[0]-B.shape[0]) | (indices_A[0] >= norm_adj.shape[0]) | \\\n",
    "           (indices_A[1] < norm_adj.shape[0]-B.shape[0]) | (indices_A[1] >= norm_adj.shape[0])\n",
    "    values_A = values_A[mask]\n",
    "    indices_A = indices_A[:, mask]\n",
    "    new_values = torch.cat([values_A, values_B], dim=0)\n",
    "    new_indices = torch.cat([indices_A, new_indices_B], dim=1)\n",
    "    new_A = torch.sparse_coo_tensor(new_indices, new_values, A.size())\n",
    "    return new_A\n",
    "\n",
    "\n",
    "\n",
    "class PositiveLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, seed):\n",
    "        super(PositiveLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.log_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.seed = seed\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        nn.init.xavier_uniform_(self.log_weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return nn.functional.linear(input, self.log_weight.exp())\n",
    "\n",
    "class NegativeLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, seed):\n",
    "        super(NegativeLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.log_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.seed = seed\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        nn.init.xavier_uniform_(self.log_weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return nn.functional.linear(input, -self.log_weight.exp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde7ae13-bbe7-401a-a2e0-f4d1bd247bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MICRO(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats,rating):\n",
    "        super().__init__()\n",
    "        self.num_envs = 2\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weight_size = weight_size\n",
    "        self.n_ui_layers = len(self.weight_size)\n",
    "        self.weight_size = [self.embedding_dim] + self.weight_size\n",
    "        self.user_embedding = nn.Embedding(n_users, self.embedding_dim)\n",
    "        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim).to('cuda')\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n",
    "        self.rating=compute_normalized_laplacian(rating.to_dense()).to_sparse().to('cuda')\n",
    "        \n",
    "        if args.cf_model == 'ngcf':\n",
    "            self.GC_Linear_list = nn.ModuleList()\n",
    "            self.Bi_Linear_list = nn.ModuleList()\n",
    "            self.dropout_list = nn.ModuleList()\n",
    "            for i in range(self.n_ui_layers):\n",
    "                self.GC_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
    "                self.Bi_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n",
    "                self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
    "        \n",
    "        \n",
    "        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False).to('cuda')\n",
    "        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False).to('cuda')\n",
    "        self.image_feats_norm = self.image_embedding.weight.detach().div(torch.norm(self.image_embedding.weight.detach(), p=2, dim=-1, keepdim=True))\n",
    "        self.text_feats_norm = self.text_embedding.weight.detach().div(torch.norm(self.text_embedding.weight.detach(), p=2, dim=-1, keepdim=True))\n",
    "\n",
    "        self.image_sim = build_sim(self.image_embedding.weight.detach())\n",
    "        self.text_sim = build_sim(self.text_embedding.weight.detach())\n",
    "        \n",
    "\n",
    "        #ini_image_adj = build_sim(self.image_embedding.weight.detach()) \n",
    "        #ini_image_adj = build_knn_normalized_graph(ini_image_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "        #ini_text_adj = build_sim(self.text_embedding.weight.detach()) \n",
    "        #ini_text_adj = build_knn_normalized_graph(ini_text_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "        ii=build_knn_graph(torch.sparse.mm(self.rating.T,self.rating).to_dense(), 50, True, norm_type='sym')\n",
    "        ii=get_dense_norm_rowandcol(ii)\n",
    "        #self.an_encoder=anomaly_encoder(ini_text_adj.clone(),ini_image_adj.clone(),image_feats.shape[1],args.embed_size).to('cuda')\n",
    "        #self.an_decoder=anomaly_decoder(ini_text_adj.clone(),ini_image_adj.clone(),image_feats.shape[1],args.embed_size).to('cuda')\n",
    "        \n",
    "        self.an_encoder=anomaly_encoder(ii,image_feats.shape[1],image_feats.shape[1]).to('cuda')\n",
    "        self.an_decoder=anomaly_decoder(ii,image_feats.shape[1],image_feats.shape[1]).to('cuda')\n",
    "        \n",
    "        self.image_trs = nn.Sequential(\n",
    "            nn.Linear(4 * image_feats.shape[1], 2 * args.embed_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * self.embedding_dim, args.embed_size, bias=False)\n",
    "        )\n",
    "        self.text_trs = nn.Sequential(\n",
    "            nn.Linear(4 * text_feats.shape[1], 2 * args.embed_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * self.embedding_dim, args.embed_size, bias=False)\n",
    "        )\n",
    "\n",
    "        self.hybird_trs = nn.Linear(4*text_feats.shape[1], args.embed_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "        self.score = nn.Sequential(\n",
    "            nn.Linear(text_feats.shape[1], self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        '''\n",
    "        self.mon_score = nn.Sequential(\n",
    "            PositiveLinear(text_feats.shape[1], self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            NegativeLinear(self.embedding_dim, 1)\n",
    "        )\n",
    "        '''\n",
    "        self.mon_score_lsit = []\n",
    "        seed=[1,100,1000,10000,100000]\n",
    "        for i in range(self.num_envs):\n",
    "            self.mon_score_lsit.append(nn.Sequential(\n",
    "                PositiveLinear(text_feats.shape[1], self.embedding_dim,seed[i]),\n",
    "                nn.Tanh(),\n",
    "                NegativeLinear(self.embedding_dim, 1,seed[i])\n",
    "                ).to('cuda'))\n",
    "        \n",
    "        self.score_ = nn.Sequential(\n",
    "            nn.Linear(image_feats.shape[1], self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.query = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.query_1 = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)\n",
    "        )\n",
    "        self.dropout_list = nn.ModuleList()\n",
    "        for i in range(self.n_ui_layers):\n",
    "            self.dropout_list.append(nn.Dropout(dropout_list[i]))\n",
    "\n",
    "        self.tau = 0.5\n",
    "        self.modal_weight = nn.Parameter(torch.Tensor([[0.5, 0.5]]))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "        self.env_generators = {}\n",
    "        for k in range(self.num_envs):\n",
    "            self.env_generators[k] = self.generator_model()\n",
    "        \n",
    "    def mm(self, x, y):\n",
    "        if args.sparse:\n",
    "            return torch.sparse.mm(x, y)\n",
    "        else:\n",
    "            return torch.mm(x, y)\n",
    "    \n",
    "    def sim(self, z1, z2):\n",
    "        z1 = F.normalize(z1)\n",
    "        z2 = F.normalize(z2)\n",
    "        return torch.mm(z1, z2.t())\n",
    "\n",
    "    def batched_contrastive_loss(self, z1, z2, batch_size=256):\n",
    "        device = z1.device\n",
    "        num_nodes = z1.size(0)\n",
    "        num_batches = (num_nodes - 1) // batch_size + 1\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "        indices = torch.arange(0, num_nodes).to(device)\n",
    "        losses = []\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            mask = indices[i * batch_size:(i + 1) * batch_size]\n",
    "            refl_sim = f(self.sim(z1[mask], z1))  # [B, N]\n",
    "            between_sim = f(self.sim(z1[mask], z2))  # [B, N]\n",
    "\n",
    "            losses.append(-torch.log(\n",
    "                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n",
    "                / (refl_sim.sum(1) + between_sim.sum(1)\n",
    "                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n",
    "                   \n",
    "        loss_vec = torch.cat(losses)\n",
    "        return loss_vec.mean()\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "    def generator_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(1, self.embedding_dim),  # 输入维度为self.latent_dim，输出维度为1\n",
    "            nn.ReLU(),  # 激活函数\n",
    "            # nn.Dropout(p=0.5),  # 如果需要Dropout层，可以取消注释\n",
    "            # nn.Linear(int(self.latent_dim / 2), 1),  # 如果需要额外的Dense层，可以取消注释\n",
    "            # nn.ReLU(),  # 激活函数\n",
    "            # nn.Dropout(p=0.5)  # 如果需要Dropout层，可以取消注释\n",
    "            nn.Linear(self.embedding_dim, 1, bias=False)  # 输出层\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def multi_env(self,rel):\n",
    "        #logit = self.env_generators[k](rel)\n",
    "        logit = rel.view(-1)\n",
    "        # bias = 0.0 + 0.0001\n",
    "        # eps = (bias - (1 - bias)) * torch.rand(logit.shape) + (1 - bias)\n",
    "        eps = torch.rand(logit.shape, device=logit.device)\n",
    "        mask_gate_input = torch.log(eps) - torch.log(1 - eps)\n",
    "        mask_gate_input = (logit + mask_gate_input) / 0.2\n",
    "        mask_gate_input = torch.sigmoid(mask_gate_input) + 0.5  # self.edge_bias\n",
    "        return mask_gate_input\n",
    "\n",
    "    \n",
    "    def update_graph(self,image_sim,text_sim,img_rel,text_rel):\n",
    "        print('time to update graph')\n",
    "        image_adj = build_rel_knn_graph(image_sim, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type,rel=img_rel)\n",
    "        #print(self.image_a.to_dense())\n",
    "        text_adj = build_rel_knn_graph(text_sim, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type,rel=text_rel)\n",
    "        text_original_adj = text_adj.cuda()\n",
    "        image_original_adj = image_adj.cuda()\n",
    "        self.synthesis_adj=cal_sum_lap(torch.tensor([[1,1]]).cuda(),[image_adj,text_adj])\n",
    "        return get_dense_norm_rowandcol(image_original_adj),get_dense_norm_rowandcol(text_original_adj),self.synthesis_adj\n",
    "\n",
    "    def forward(self, adj, step_count,build_item_graph=False):\n",
    "        text_rel_list = []\n",
    "        img_rel_list = []\n",
    "        \n",
    "        u_g_embeddings_list = []\n",
    "        i_g_embeddings_list = []\n",
    "        image_item_embeds_list = []\n",
    "        text_item_embeds_list = []\n",
    "        h_list = []\n",
    "        \n",
    "        img_latent,text_latent=self.an_encoder(self.image_feats_norm,self.text_feats_norm)\n",
    "        img_rec,text_rec=self.an_decoder(img_latent,text_latent)\n",
    "        \n",
    "        #text_res=text_rec-self.text_feats_norm\n",
    "        #img_res=img_rec-self.image_feats_norm\n",
    "        #text_rel = self.mon_score((text_rec-self.text_feats_norm)*(text_rec-self.text_feats_norm))\n",
    "        #img_rel = self.mon_score((img_rec-self.image_feats_norm)*(img_rec-self.image_feats_norm))\n",
    "        \n",
    "        #print('diag',diag_i.shape)\n",
    "        img_rec_loss = ((img_rec-self.image_feats_norm)*(img_rec-self.image_feats_norm)).sum(dim=-1).reshape((1,-1))\n",
    "        text_rec_loss = ((text_rec-self.text_feats_norm)*(text_rec-self.text_feats_norm)).sum(dim=-1).reshape((1,-1))\n",
    "\n",
    "        for i in range(self.num_envs):\n",
    "            text_rel=self.mon_score_lsit[i]((text_rec-self.text_feats_norm)*(text_rec-self.text_feats_norm))\n",
    "            #text_rel=self.mon_score_lsit[i](text_rec-self.text_feats_norm)\n",
    "            img_rel=self.mon_score_lsit[i]((img_rec-self.image_feats_norm)*(img_rec-self.image_feats_norm))\n",
    "            #img_rel=self.mon_score_lsit[i](img_rec-self.image_feats_norm)\n",
    "            text_rel_list.append(text_rel)\n",
    "            img_rel_list.append(img_rel)\n",
    "        #print(text_rel_list)\n",
    "        #text_rel_list=self.multi_env(text_rel)\n",
    "        #img_rel_list=self.multi_env(img_rel)\n",
    "        \n",
    "        for k in range(self.num_envs):\n",
    "            text_rel = self.multi_env(text_rel_list[k])\n",
    "            img_rel = self.multi_env(img_rel_list[k])\n",
    "            if step_count%50==0:\n",
    "                self.image_original_adj,self.text_original_adj,synthesis_adj= self.update_graph(self.image_sim,self.text_sim,\n",
    "                                                                                                img_rel,text_rel)\n",
    "            else:\n",
    "                synthesis_adj=self.synthesis_adj.detach()\n",
    "    \n",
    "            image_feats = self.image_trs(torch.cat((img_rec,self.image_embedding.weight,\n",
    "                                                    self.image_embedding.weight+img_rec,self.image_embedding.weight*img_rec),dim=1))\n",
    "            text_feats = self.text_trs(torch.cat((text_rec,self.text_embedding.weight,\n",
    "                                                    self.text_embedding.weight+text_rec,self.text_embedding.weight*text_rec),dim=1))\n",
    "            if step_count%200==0:\n",
    "                self.image_adj = build_sim(image_feats) \n",
    "                self.image_adj = build_knn_graph(self.image_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "                self.image_adj = get_dense_norm_rowandcol(self.image_adj)\n",
    "                self.image_adj = (1 - args.lambda_coeff) * self.image_adj + args.lambda_coeff * self.image_original_adj\n",
    "                self.text_adj = build_sim(text_feats) \n",
    "                self.text_adj = build_knn_graph(self.text_adj, topk=args.topk, is_sparse=args.sparse, norm_type=args.norm_type)\n",
    "                self.text_adj = get_dense_norm_rowandcol(self.text_adj)\n",
    "                self.text_adj = (1 - args.lambda_coeff) * self.text_adj + args.lambda_coeff * self.text_original_adj\n",
    "            else:\n",
    "                self.image_adj = self.image_adj.detach()\n",
    "                self.text_adj = self.text_adj.detach()\n",
    "            image_item_embeds = self.item_id_embedding.weight\n",
    "            text_item_embeds = self.item_id_embedding.weight\n",
    "    \n",
    "            for i in range(args.layers):\n",
    "                image_item_embeds = self.mm(self.image_adj, image_item_embeds)\n",
    "                text_item_embeds = self.mm(self.text_adj, text_item_embeds)\n",
    "    \n",
    "            att = torch.cat([self.query(image_item_embeds), self.query(text_item_embeds)], dim=-1)\n",
    "            weight = self.softmax(att)\n",
    "            h = weight[:, 0].unsqueeze(dim=1) * (image_item_embeds) + weight[:,1].unsqueeze(dim=1) * (text_item_embeds)\n",
    "            \n",
    "            uii_i_emb=torch.sparse.mm(self.rating, image_item_embeds)\n",
    "            uii_t_emb=torch.sparse.mm(self.rating, text_item_embeds)\n",
    "            image_user_feats=torch.sparse.mm(self.rating, image_feats)\n",
    "            text_user_feats=torch.sparse.mm(self.rating, text_feats)\n",
    "            \n",
    "            att_ = torch.cat([self.query_1(uii_i_emb), self.query_1(uii_t_emb)], dim=-1)\n",
    "            modal_weight = self.softmax(att_)\n",
    "            hh=modal_weight[:, 0].unsqueeze(dim=1) * uii_i_emb + modal_weight[:, 1].unsqueeze(dim=1) * uii_t_emb\n",
    "            \n",
    "            ego_embeddings_u=self.user_embedding.weight\n",
    "            ego_embeddings_i=self.item_id_embedding.weight\n",
    "            user_embeddings=[self.user_embedding.weight]\n",
    "            item_embeddings=[self.item_id_embedding.weight]\n",
    "            \n",
    "            for i in range(self.n_ui_layers):\n",
    "                side_embeddings_u = torch.sparse.mm(self.rating, ego_embeddings_i)\n",
    "                side_embeddings_u=self.dropout_list[i](side_embeddings_u)\n",
    "                \n",
    "    \n",
    "                side_embeddings_i = torch.sparse.mm(self.rating.T, ego_embeddings_u) + torch.sparse.mm(synthesis_adj, ego_embeddings_i)\n",
    "                side_embeddings_i=self.dropout_list[i](side_embeddings_i)\n",
    "                \n",
    "                ego_embeddings_u = side_embeddings_u\n",
    "                user_embeddings += [ego_embeddings_u]\n",
    "                ego_embeddings_i = side_embeddings_i\n",
    "                item_embeddings += [ego_embeddings_i]\n",
    "            user_embeddings = torch.stack(user_embeddings, dim=1)\n",
    "            user_embeddings = user_embeddings.mean(dim=1, keepdim=False)\n",
    "            \n",
    "            item_embeddings = torch.stack(item_embeddings, dim=1)\n",
    "            item_embeddings = item_embeddings.mean(dim=1, keepdim=False)\n",
    "            \n",
    "            i_g_embeddings = item_embeddings\n",
    "            u_g_embeddings = user_embeddings\n",
    "            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n",
    "            u_g_embeddings = u_g_embeddings + F.normalize(hh, p=2, dim=1)\n",
    "            u_g_embeddings_list.append(u_g_embeddings)\n",
    "            i_g_embeddings_list.append(i_g_embeddings)\n",
    "            image_item_embeds_list.append(image_item_embeds)\n",
    "            text_item_embeds_list.append(text_item_embeds)\n",
    "            h_list.append(h)\n",
    "        final_u_g_embeddings = torch.stack(u_g_embeddings_list).mean(dim=0)\n",
    "        final_i_g_embeddings = torch.stack(i_g_embeddings_list).mean(dim=0)\n",
    "        final_image_item_embeds = torch.stack(image_item_embeds_list).mean(dim=0)\n",
    "        final_text_item_embeds = torch.stack(text_item_embeds_list).mean(dim=0)\n",
    "        final_h = torch.stack(h_list).mean(dim=0)\n",
    "        #return final_u_g_embeddings, final_i_g_embeddings, final_image_item_embeds, final_text_item_embeds,\\\n",
    "        #    final_h, img_rec_loss,text_rec_loss\n",
    "        return u_g_embeddings_list, i_g_embeddings_list, final_image_item_embeds, final_text_item_embeds,\\\n",
    "            final_h, img_rec_loss,text_rec_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2683a438-f619-4919-93df-25e6106b3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (26495, 26495) 0.029147624969482422\n",
      "already load rating matrix (19445, 7050) 0.005362987518310547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4934/2920664697.py:290: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../baby/clip_shuffled_0_image_feats.npy\n",
      "../baby/clip_shuffled_0_text_feats.npy\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 0 [22.8s]: train==[464.12085=58.86221 + 0.00187 + 0.00000 + 55.06337 + 350.19333 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 1 [24.0s]: train==[127.93129=58.99597 + 0.00187 + 0.00000 + 55.06398 + 13.86947 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 2 [28.5s]: train==[122.60332=54.90937 + 0.00197 + 0.00000 + 54.61876 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 3 [29.1s]: train==[115.96183=48.58265 + 0.00224 + 0.00000 + 54.30373 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 4 [28.8s]: train==[111.20647=43.88297 + 0.00254 + 0.00000 + 54.24774 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 5 [29.1s]: train==[107.25826=39.95629 + 0.00288 + 0.00000 + 54.22587 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 6 [29.2s]: train==[104.11613=36.84363 + 0.00321 + 0.00000 + 54.19607 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 7 [29.0s]: train==[101.17692=33.94265 + 0.00357 + 0.00000 + 54.15748 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 8 [29.4s]: train==[98.78207=31.58285 + 0.00391 + 0.00000 + 54.12209 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 9 [29.0s]: train==[96.52433=29.37059 + 0.00426 + 0.00000 + 54.07627 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 10 [28.9s]: train==[94.64121=27.52848 + 0.00459 + 0.00000 + 54.03492 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 10 [28.9s + 9.3s]:  val==[94.64121=27.52848 + 0.00459 + 0.00000], recall=[0.05726, 0.08943], precision=[0.00633, 0.00495], hit=[0.06274, 0.09787], ndcg=[0.03310, 0.04191], mrr=[0.02467, 0.02705]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 10 [28.9s + 9.3s]: test==[94.64121=27.52848 + 0.00459 + 0.00000], recall=[0.05673, 0.08829], precision=[0.00596, 0.00461], hit=[0.05945, 0.09180], ndcg=[0.03069, 0.03883], mrr=[0.02227, 0.02448]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 11 [28.6s]: train==[92.90528=25.82494 + 0.00493 + 0.00000 + 54.00219 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 12 [29.3s]: train==[91.29458=24.24747 + 0.00526 + 0.00000 + 53.96863 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 13 [28.6s]: train==[89.85898=22.84211 + 0.00558 + 0.00000 + 53.93806 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 14 [29.2s]: train==[88.77557=21.78991 + 0.00588 + 0.00000 + 53.90656 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 15 [28.6s]: train==[87.56703=20.61139 + 0.00618 + 0.00000 + 53.87624 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 15 [28.6s + 9.5s]:  val==[87.56703=20.61139 + 0.00618 + 0.00000], recall=[0.05942, 0.09265], precision=[0.00657, 0.00513], hit=[0.06526, 0.10126], ndcg=[0.03418, 0.04328], mrr=[0.02540, 0.02785]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 15 [28.6s + 9.5s]: test==[87.56703=20.61139 + 0.00618 + 0.00000], recall=[0.05960, 0.09119], precision=[0.00624, 0.00477], hit=[0.06233, 0.09488], ndcg=[0.03192, 0.04012], mrr=[0.02304, 0.02526]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 16 [28.6s]: train==[86.36682=19.43636 + 0.00648 + 0.00000 + 53.85076 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 17 [28.8s]: train==[85.37088=18.46260 + 0.00676 + 0.00000 + 53.82830 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 18 [29.0s]: train==[84.53772=17.65050 + 0.00705 + 0.00000 + 53.80695 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 19 [28.6s]: train==[83.67329=16.80723 + 0.00731 + 0.00000 + 53.78553 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 20 [29.3s]: train==[82.83648=15.99071 + 0.00759 + 0.00000 + 53.76496 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 20 [29.3s + 8.9s]:  val==[82.83648=15.99071 + 0.00759 + 0.00000], recall=[0.06080, 0.09473], precision=[0.00671, 0.00525], hit=[0.06660, 0.10373], ndcg=[0.03494, 0.04424], mrr=[0.02604, 0.02854]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 20 [29.3s + 8.9s]: test==[82.83648=15.99071 + 0.00759 + 0.00000], recall=[0.06008, 0.09399], precision=[0.00628, 0.00492], hit=[0.06264, 0.09787], ndcg=[0.03221, 0.04106], mrr=[0.02330, 0.02569]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 21 [29.1s]: train==[82.07270=15.24359 + 0.00784 + 0.00000 + 53.74805 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 22 [28.9s]: train==[81.49356=14.68015 + 0.00810 + 0.00000 + 53.73209 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 23 [29.0s]: train==[80.76910=13.97252 + 0.00834 + 0.00000 + 53.71502 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 24 [29.0s]: train==[80.26769=13.48562 + 0.00858 + 0.00000 + 53.70026 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 25 [28.7s]: train==[79.66641=12.89775 + 0.00883 + 0.00000 + 53.68661 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 25 [28.7s + 9.5s]:  val==[79.66641=12.89775 + 0.00883 + 0.00000], recall=[0.06179, 0.09563], precision=[0.00682, 0.00529], hit=[0.06773, 0.10455], ndcg=[0.03541, 0.04470], mrr=[0.02633, 0.02886]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 25 [28.7s + 9.5s]: test==[79.66641=12.89775 + 0.00883 + 0.00000], recall=[0.06159, 0.09602], precision=[0.00645, 0.00503], hit=[0.06434, 0.09987], ndcg=[0.03299, 0.04190], mrr=[0.02387, 0.02625]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 26 [28.6s]: train==[79.15939=12.40359 + 0.00907 + 0.00000 + 53.67351 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 27 [28.9s]: train==[78.63791=11.89464 + 0.00928 + 0.00000 + 53.66077 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 28 [28.7s]: train==[78.10371=11.37294 + 0.00948 + 0.00000 + 53.64808 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 29 [28.8s]: train==[77.72045=11.00197 + 0.00972 + 0.00000 + 53.63555 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 30 [29.0s]: train==[77.30715=10.59846 + 0.00994 + 0.00000 + 53.62552 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 30 [29.0s + 9.7s]:  val==[77.30715=10.59846 + 0.00994 + 0.00000], recall=[0.06235, 0.09700], precision=[0.00690, 0.00537], hit=[0.06850, 0.10625], ndcg=[0.03554, 0.04505], mrr=[0.02630, 0.02888]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 30 [29.0s + 9.7s]: test==[77.30715=10.59846 + 0.00994 + 0.00000], recall=[0.06165, 0.09631], precision=[0.00646, 0.00504], hit=[0.06444, 0.10023], ndcg=[0.03309, 0.04209], mrr=[0.02401, 0.02643]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 31 [29.0s]: train==[76.94902=10.25040 + 0.01016 + 0.00000 + 53.61524 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 32 [28.5s]: train==[76.44787=9.75931 + 0.01037 + 0.00000 + 53.60496 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 33 [29.3s]: train==[76.05247=9.37511 + 0.01059 + 0.00000 + 53.59356 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 34 [28.6s]: train==[75.79533=9.12649 + 0.01078 + 0.00000 + 53.58484 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 35 [29.0s]: train==[75.46775=8.80839 + 0.01098 + 0.00000 + 53.57516 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 35 [29.0s + 9.4s]:  val==[75.46775=8.80839 + 0.01098 + 0.00000], recall=[0.06311, 0.09788], precision=[0.00699, 0.00543], hit=[0.06948, 0.10712], ndcg=[0.03575, 0.04530], mrr=[0.02627, 0.02884]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 35 [29.0s + 9.4s]: test==[75.46775=8.80839 + 0.01098 + 0.00000], recall=[0.06272, 0.09610], precision=[0.00659, 0.00504], hit=[0.06562, 0.10003], ndcg=[0.03368, 0.04232], mrr=[0.02440, 0.02672]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 36 [28.6s]: train==[75.21344=8.56260 + 0.01118 + 0.00000 + 53.56645 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 37 [28.6s]: train==[74.84668=8.20313 + 0.01138 + 0.00000 + 53.55895 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 38 [28.9s]: train==[74.61071=7.97453 + 0.01159 + 0.00000 + 53.55136 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 39 [29.1s]: train==[74.40450=7.77565 + 0.01173 + 0.00000 + 53.54390 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 40 [28.6s]: train==[74.04168=7.42191 + 0.01194 + 0.00000 + 53.53461 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 40 [28.6s + 9.6s]:  val==[74.04168=7.42191 + 0.01194 + 0.00000], recall=[0.06314, 0.09755], precision=[0.00696, 0.00541], hit=[0.06927, 0.10676], ndcg=[0.03588, 0.04537], mrr=[0.02651, 0.02909]\n",
      "#####Early stopping steps: 1 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 41 [28.7s]: train==[73.78932=7.17635 + 0.01215 + 0.00000 + 53.52760 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 42 [29.1s]: train==[73.61368=7.00825 + 0.01232 + 0.00000 + 53.51988 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 43 [28.7s]: train==[73.37503=6.77593 + 0.01255 + 0.00000 + 53.51333 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 44 [29.3s]: train==[73.11447=6.52282 + 0.01271 + 0.00000 + 53.50572 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 45 [28.6s]: train==[72.95041=6.36524 + 0.01291 + 0.00000 + 53.49903 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 45 [28.6s + 9.5s]:  val==[72.95041=6.36524 + 0.01291 + 0.00000], recall=[0.06333, 0.09758], precision=[0.00698, 0.00541], hit=[0.06938, 0.10681], ndcg=[0.03563, 0.04507], mrr=[0.02616, 0.02871]\n",
      "#####Early stopping steps: 2 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 46 [28.6s]: train==[72.80745=6.22928 + 0.01303 + 0.00000 + 53.49192 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 47 [28.9s]: train==[72.52181=5.94987 + 0.01326 + 0.00000 + 53.48546 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 48 [28.4s]: train==[72.39894=5.83337 + 0.01341 + 0.00000 + 53.47894 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 49 [28.3s]: train==[72.26754=5.70731 + 0.01361 + 0.00000 + 53.47340 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 50 [29.1s]: train==[72.12978=5.57513 + 0.01374 + 0.00000 + 53.46769 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 50 [29.1s + 9.4s]:  val==[72.12978=5.57513 + 0.01374 + 0.00000], recall=[0.06311, 0.09704], precision=[0.00698, 0.00537], hit=[0.06938, 0.10615], ndcg=[0.03581, 0.04506], mrr=[0.02641, 0.02890]\n",
      "#####Early stopping steps: 3 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 51 [28.3s]: train==[71.93209=5.38303 + 0.01388 + 0.00000 + 53.46196 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 52 [28.3s]: train==[71.80010=5.25729 + 0.01409 + 0.00000 + 53.45550 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 53 [28.7s]: train==[71.63995=5.10211 + 0.01425 + 0.00000 + 53.45037 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 54 [28.3s]: train==[71.47761=4.94537 + 0.01439 + 0.00000 + 53.44462 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 55 [28.7s]: train==[71.38767=4.86106 + 0.01456 + 0.00000 + 53.43884 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 55 [28.7s + 8.9s]:  val==[71.38767=4.86106 + 0.01456 + 0.00000], recall=[0.06304, 0.09730], precision=[0.00696, 0.00539], hit=[0.06917, 0.10635], ndcg=[0.03564, 0.04502], mrr=[0.02627, 0.02878]\n",
      "#####Early stopping steps: 4 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 56 [28.9s]: train==[71.34704=4.82575 + 0.01471 + 0.00000 + 53.43336 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 57 [28.6s]: train==[71.10889=4.59199 + 0.01486 + 0.00000 + 53.42882 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 58 [28.8s]: train==[71.03280=4.52138 + 0.01499 + 0.00000 + 53.42321 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 59 [28.5s]: train==[70.87644=4.37021 + 0.01518 + 0.00000 + 53.41783 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 60 [28.7s]: train==[70.78756=4.28692 + 0.01535 + 0.00000 + 53.41208 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 60 [28.7s + 9.1s]:  val==[70.78756=4.28692 + 0.01535 + 0.00000], recall=[0.06369, 0.09711], precision=[0.00706, 0.00538], hit=[0.07015, 0.10615], ndcg=[0.03599, 0.04509], mrr=[0.02651, 0.02894]\n",
      "#####Early stopping steps: 5 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 61 [29.1s]: train==[70.71684=4.22026 + 0.01547 + 0.00000 + 53.40788 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 62 [28.3s]: train==[70.67486=4.18301 + 0.01564 + 0.00000 + 53.40299 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 63 [28.6s]: train==[70.46356=3.97628 + 0.01575 + 0.00000 + 53.39832 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 64 [28.7s]: train==[70.31135=3.82904 + 0.01591 + 0.00000 + 53.39318 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 65 [28.5s]: train==[70.26280=3.78450 + 0.01606 + 0.00000 + 53.38902 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 65 [28.5s + 9.3s]:  val==[70.26280=3.78450 + 0.01606 + 0.00000], recall=[0.06232, 0.09641], precision=[0.00692, 0.00535], hit=[0.06876, 0.10558], ndcg=[0.03545, 0.04478], mrr=[0.02617, 0.02868]\n",
      "#####Early stopping steps: 6 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 66 [29.0s]: train==[70.17510=3.70111 + 0.01625 + 0.00000 + 53.38452 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 67 [28.7s]: train==[70.10350=3.63412 + 0.01637 + 0.00000 + 53.37980 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 68 [28.9s]: train==[70.06185=3.59681 + 0.01652 + 0.00000 + 53.37530 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 69 [29.0s]: train==[69.94719=3.48614 + 0.01667 + 0.00000 + 53.37116 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 70 [28.3s]: train==[69.82976=3.37380 + 0.01680 + 0.00000 + 53.36594 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 70 [28.3s + 9.6s]:  val==[69.82976=3.37380 + 0.01680 + 0.00000], recall=[0.06185, 0.09632], precision=[0.00686, 0.00536], hit=[0.06824, 0.10553], ndcg=[0.03520, 0.04468], mrr=[0.02604, 0.02860]\n",
      "#####Early stopping steps: 7 #####\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 71 [28.8s]: train==[69.72546=3.27390 + 0.01695 + 0.00000 + 53.36139 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 72 [28.8s]: train==[69.77645=3.32853 + 0.01709 + 0.00000 + 53.35762 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 73 [28.4s]: train==[69.70475=3.26031 + 0.01722 + 0.00000 + 53.35401 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 74 [29.3s]: train==[69.57941=3.13887 + 0.01734 + 0.00000 + 53.34997 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 75 [28.5s]: train==[69.53781=3.10152 + 0.01748 + 0.00000 + 53.34560 + 13.07323 + 0.00000]\n",
      "time to update graph\n",
      "time to update graph\n",
      "Epoch 75 [28.5s + 9.1s]:  val==[69.53781=3.10152 + 0.01748 + 0.00000], recall=[0.06159, 0.09664], precision=[0.00685, 0.00537], hit=[0.06804, 0.10594], ndcg=[0.03538, 0.04500], mrr=[0.02628, 0.02887]\n",
      "#####Early stop! #####\n",
      "{'precision': array([0.00658781, 0.00503728, 0.00341785]), 'recall': array([0.06272111, 0.0961033 , 0.16190733]), 'ndcg': array([0.03367626, 0.04232206, 0.05595629]), 'hit_ratio': array([0.06562098, 0.10002571, 0.16832091]), 'mrr': array([0.02440232, 0.02671648, 0.02885026]), 'auc': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time as tm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.sparse as sparse\n",
    "\n",
    "#from utility.parser import parse_args\n",
    "#from Models import LATTICE\n",
    "#from utility.batch_test import *\n",
    "\n",
    "args = parse_args()\n",
    "rec_epoch=2\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, data_config):\n",
    "        # argument settings\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "\n",
    "        self.model_name = args.model_name\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.lr = args.lr\n",
    "        self.emb_dim = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.weight_size = eval(args.weight_size)\n",
    "        self.n_layers = len(self.weight_size)\n",
    "        self.regs = eval(args.regs)\n",
    "        self.decay = self.regs[0]\n",
    "\n",
    "        self.norm_adj = data_config['norm_adj']\n",
    "        self.norm_adj = self.sparse_mx_to_torch_sparse_tensor(self.norm_adj).float().cuda()\n",
    "        self.rating=data_config['rating']\n",
    "        self.rating = self.sparse_mx_to_torch_sparse_tensor(self.rating).float().cuda()\n",
    "        if args.shuffle=='all':\n",
    "            image_feats = np.load(args.data_path+'{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load(args.data_path+'{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print(args.data_path+'{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print(args.data_path+'{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='text':\n",
    "            image_feats = np.load(args.data_path+'{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load(args.data_path+'{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "            print(args.data_path+'{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print(args.data_path+'{}/clip_shuffled_{}_text_feats.npy'.format(args.dataset,args.p))\n",
    "        elif args.shuffle=='image':\n",
    "            image_feats = np.load(args.data_path+'{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            text_feats = np.load(args.data_path+'{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print(args.data_path+'{}/clip_shuffled_{}_image_feats.npy'.format(args.dataset,args.p))\n",
    "            print(args.data_path+'{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        else:\n",
    "            image_feats = np.load(args.data_path+'{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            text_feats = np.load(args.data_path+'{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "            print(args.data_path+'{}/clip_shuffled_0_image_feats.npy'.format(args.dataset))\n",
    "            print(args.data_path+'{}/clip_shuffled_0_text_feats.npy'.format(args.dataset))\n",
    "        self.model = MICRO(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, image_feats, text_feats, self.rating)\n",
    "        self.model = self.model.cuda()\n",
    "        \n",
    "        self.rec_optimizer = optim.Adam([{'params': self.model.an_encoder.parameters()},{'params': self.model.an_decoder.parameters()}], lr=0.01)\n",
    "        \n",
    "\n",
    "    def set_lr_scheduler(self):\n",
    "        fac = lambda epoch: 0.96 ** (epoch / 50)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)\n",
    "        return scheduler\n",
    "    def forzen_ano_train_other(self):\n",
    "        for param in self.model.an_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.an_decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def test(self, users_to_test, is_val):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            ua_embeddings, ia_embeddings, *rest = self.model(self.norm_adj,0, build_item_graph=True)\n",
    "        result = test_torch(torch.stack(ua_embeddings).mean(dim=0), torch.stack(ia_embeddings).mean(dim=0), users_to_test, is_val)\n",
    "        return result\n",
    "\n",
    "    def train(self):\n",
    "        training_time_list = []\n",
    "        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger,mrr_loger = [], [], [], [], [], []\n",
    "        stopping_step = 0\n",
    "        should_stop = False\n",
    "        cur_best_pre_0 = 0.\n",
    "\n",
    "        n_batch = data_generator.n_train // args.batch_size + 1\n",
    "        best_recall = 0\n",
    "        for epoch in (range(args.epoch)):\n",
    "            if epoch==rec_epoch:\n",
    "                self.optimizer=self.forzen_ano_train_other()\n",
    "                self.lr_scheduler = self.set_lr_scheduler()\n",
    "            t1 = time()\n",
    "            loss, mf_loss, emb_loss, reg_loss,mf_var_loss = 0., 0., 0., 0.,0.\n",
    "            l_rec_loss= 0. \n",
    "            contrastive_loss = 0.\n",
    "            n_batch = data_generator.n_train // args.batch_size + 1\n",
    "            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.\n",
    "            sample_time = 0.\n",
    "            build_item_graph = True\n",
    "            for idx in (range(n_batch)):\n",
    "                #print('batch num ',idx)\n",
    "                self.model.train()\n",
    "                torch.autograd.set_detect_anomaly(False)\n",
    "                if epoch>=rec_epoch:\n",
    "                    self.optimizer.zero_grad()\n",
    "                self.rec_optimizer.zero_grad()\n",
    "                sample_t1 = time()\n",
    "                users, pos_items, neg_items = data_generator.sample()\n",
    "                sample_time += time() - sample_t1\n",
    "                #if idx%50==0:\n",
    "                ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, fusion_embed, img_rel,text_rel = self.model(self.norm_adj,idx, build_item_graph=build_item_graph)\n",
    "                #else:\n",
    "                    #ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, fusion_embed = self.model(self.norm_adj,idx, build_item_graph=build_item_graph)\n",
    "                #build_item_graph = False\n",
    "                #u_g_embeddings = ua_embeddings[users]\n",
    "                #pos_i_g_embeddings = ia_embeddings[pos_items]\n",
    "                #neg_i_g_embeddings = ia_embeddings[neg_items]\n",
    "                mf_loss_tmp_list=[]\n",
    "                batch_mf_loss = 0\n",
    "                batch_emb_loss = 0\n",
    "                batch_reg_loss = 0\n",
    "                for k in range(len(ua_embeddings)):\n",
    "                    \n",
    "                    u_g_embeddings = ua_embeddings[k][users]\n",
    "                    pos_i_g_embeddings = ia_embeddings[k][pos_items]\n",
    "                    neg_i_g_embeddings = ia_embeddings[k][neg_items]\n",
    "                    en_batch_mf_loss, en_batch_emb_loss, en_batch_reg_loss = self.bpr_loss(u_g_embeddings, pos_i_g_embeddings,\n",
    "                                                                              neg_i_g_embeddings)\n",
    "                    batch_mf_loss+=en_batch_mf_loss\n",
    "                    batch_emb_loss+=en_batch_emb_loss\n",
    "                    batch_reg_loss+=en_batch_reg_loss\n",
    "                    mf_loss_tmp_list.append(batch_mf_loss)\n",
    "\n",
    "                \n",
    "                batch_mf_loss /= len(ua_embeddings)\n",
    "                batch_emb_loss /= len(ua_embeddings)\n",
    "                batch_reg_loss /= len(ua_embeddings)\n",
    "                #print(mf_loss_tmp_list)\n",
    "                if len(mf_loss_tmp_list)>1:\n",
    "                    batch_mf_var_loss=torch.var(torch.stack(mf_loss_tmp_list))\n",
    "                else:\n",
    "                    batch_mf_var_loss=0.\n",
    "                #print(batch_mf_var_loss)\n",
    "                \n",
    "                batch_contrastive_loss = 0\n",
    "                batch_contrastive_loss += self.model.batched_contrastive_loss(image_item_embeds,fusion_embed)\n",
    "                batch_contrastive_loss += self.model.batched_contrastive_loss(text_item_embeds,fusion_embed)\n",
    "\n",
    "                #batch_contrastive_loss += self.model.batched_contrastive_loss(uii_i_emb,hh)\n",
    "                #batch_contrastive_loss += self.model.batched_contrastive_loss(uii_t_emb,hh)\n",
    "\n",
    "                batch_l_rec_loss=(img_rel+text_rel).mean()\n",
    "                batch_l_rec_loss*=0.1\n",
    "                batch_contrastive_loss *=  args.loss_ratio\n",
    "                batch_loss = batch_mf_loss + batch_emb_loss + batch_reg_loss + batch_contrastive_loss + batch_l_rec_loss +batch_mf_var_loss\n",
    "                if epoch<rec_epoch:\n",
    "                    batch_l_rec_loss.backward(retain_graph=False)\n",
    "                    self.rec_optimizer.step()\n",
    "                else:\n",
    "                    \n",
    "                    batch_loss.backward(retain_graph=False)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                loss += float(batch_loss)\n",
    "                mf_loss += float(batch_mf_loss)\n",
    "                emb_loss += float(batch_emb_loss)\n",
    "                reg_loss += float(batch_reg_loss)\n",
    "                mf_var_loss += batch_mf_var_loss\n",
    "                contrastive_loss += float(batch_contrastive_loss)\n",
    "                l_rec_loss +=batch_l_rec_loss\n",
    "                #l_rec_loss+=0\n",
    "            if epoch>=rec_epoch:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            del ua_embeddings, ia_embeddings, u_g_embeddings, neg_i_g_embeddings, pos_i_g_embeddings\n",
    "\n",
    "            if math.isnan(loss) == True:\n",
    "                print('ERROR: loss is nan.')\n",
    "                sys.exit()\n",
    "\n",
    "            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f + %.5f + %.5f + %.5f + %.5f]' % (\n",
    "                epoch, time() - t1, loss, mf_loss, emb_loss, reg_loss, contrastive_loss, l_rec_loss,mf_var_loss)\n",
    "            training_time_list.append(time() - t1)\n",
    "            print(perf_str)\n",
    "\n",
    "            if epoch<rec_epoch+5:\n",
    "                continue\n",
    "            \n",
    "            if epoch % args.verbose != 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            t2 = time()\n",
    "            users_to_test = list(data_generator.test_set.keys())\n",
    "            users_to_val = list(data_generator.val_set.keys())\n",
    "            ret = self.test(users_to_val, is_val=True)\n",
    "            training_time_list.append(t2 - t1)\n",
    "\n",
    "            t3 = time()\n",
    "\n",
    "            loss_loger.append(loss)\n",
    "            rec_loger.append(ret['recall'])\n",
    "            pre_loger.append(ret['precision'])\n",
    "            ndcg_loger.append(ret['ndcg'])\n",
    "            hit_loger.append(ret['hit_ratio'])\n",
    "            if args.verbose > 0:\n",
    "                perf_str = 'Epoch %d [%.1fs + %.1fs]:  val==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
    "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], mrr=[%.5f, %.5f]' % \\\n",
    "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0],\n",
    "                            ret['recall'][1],\n",
    "                            ret['precision'][0], ret['precision'][1], ret['hit_ratio'][0], ret['hit_ratio'][1],\n",
    "                            ret['ndcg'][0], ret['ndcg'][1],ret['mrr'][0], ret['mrr'][1])\n",
    "                print(perf_str)\n",
    "\n",
    "            if ret['recall'][1] > best_recall:\n",
    "                best_recall = ret['recall'][1]\n",
    "                test_ret = self.test(users_to_test, is_val=False)\n",
    "                perf_str = 'Epoch %d [%.1fs + %.1fs]: test==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
    "                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], mrr=[%.5f, %.5f]' % \\\n",
    "                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, test_ret['recall'][0],\n",
    "                            test_ret['recall'][1],\n",
    "                            test_ret['precision'][0], test_ret['precision'][1], test_ret['hit_ratio'][0], test_ret['hit_ratio'][1],\n",
    "                            test_ret['ndcg'][0], test_ret['ndcg'][1],test_ret['mrr'][0], test_ret['mrr'][1])\n",
    "                print(perf_str)                \n",
    "                stopping_step = 0\n",
    "            elif stopping_step < args.early_stopping_patience:\n",
    "                stopping_step += 1\n",
    "                print('#####Early stopping steps: %d #####' % stopping_step)\n",
    "            else:\n",
    "                print('#####Early stop! #####')\n",
    "                break\n",
    "        print(test_ret)\n",
    "        print(args.dataset,args.shuffle,args.p,tm.strftime(\"%a %b %d %H:%M:%S %Y\", tm.localtime()),test_ret, file=open('test_ret.txt','a'))\n",
    "        \n",
    "\n",
    "    def bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n",
    "\n",
    "        regularizer = 1./2*(users**2).sum() + 1./2*(pos_items**2).sum() + 1./2*(neg_items**2).sum()\n",
    "        regularizer = regularizer / self.batch_size\n",
    "\n",
    "        maxi = F.logsigmoid(pos_scores - neg_scores)\n",
    "        mf_loss = -torch.mean(maxi)\n",
    "\n",
    "        emb_loss = self.decay * regularizer\n",
    "        reg_loss = 0.0\n",
    "        return mf_loss, emb_loss, reg_loss\n",
    "\n",
    "    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n",
    "        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "        indices = torch.from_numpy(\n",
    "            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "        values = torch.from_numpy(sparse_mx.data)\n",
    "        shape = torch.Size(sparse_mx.shape)\n",
    "        return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed) # cpu\n",
    "    torch.cuda.manual_seed_all(seed)  # gpu\n",
    "\n",
    "\n",
    "set_seed(args.seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "\n",
    "config = dict()\n",
    "config['n_users'] = data_generator.n_users\n",
    "config['n_items'] = data_generator.n_items\n",
    "\n",
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n",
    "\n",
    "rating=data_generator.get_R_mat()\n",
    "\n",
    "config['norm_adj'] = norm_adj\n",
    "config['rating'] = rating\n",
    "\n",
    "trainer = Trainer(data_config=config)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d9179-173c-4420-89ff-5331ce52db3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
